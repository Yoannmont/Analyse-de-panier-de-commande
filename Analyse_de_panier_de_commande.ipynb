{"cells":[{"cell_type":"markdown","metadata":{"id":"4mh2MiuVMazk"},"source":["# TP2 - Market Basket Analysis \n","INF8111 - Fouille de données, Automne 2022\n","### Membres de l'équipe / Team Components\n","    - Ronzon Floriane (2229491)\n","    - Monteiro Yoann (2235148)\n","    - Mahy Arthur (2228804)\n"]},{"cell_type":"markdown","metadata":{"id":"bh1o2bpwPQJG"},"source":["## Date et directives de remise / Delivery date and instructions\n","Vous remettrez ce fichier nommé TP2\\_NomDuMembre1\\_NomDuMembre2\\_NomDuMembre3.ipynb dans la boîte de remise sur moodle. \n","\n","**Date limite: Novembre 6 at 23:55**.\n","___\n","You should upload the file named TP2\\_nameOfMember1\\_nameOfMember2\\_nameOfMember3.ipynb on moodle.\n","\n","Everything must be uploaded before **November 6th at 23:55**."]},{"cell_type":"markdown","metadata":{"id":"VmJEz5JEMazl"},"source":["## 1. Introduction: Market Basket Analysis\n","\n","Le *Market Basket Analysis* (MBA) est une technique d'analyse de la fouille de données qui permet de découvrir les associations entre les produits ou leur regroupement. En explorant des motifs intéressants à partir d'une vaste collection de données, le MBA vise à comprendre / révéler les comportements d'achat des clients en se basant sur la théorie selon laquelle si vous avez acheté un certain ensemble de produits, vous êtes plus (ou moins) susceptible d'acheter un autre groupe de produits. En d'autres termes, le MBA permet aux détaillants d'identifier la relation entre les articles que les clients achètent, révélant des tendances d'articles souvent achetés ensemble.\n","\n","Une approche largement utilisée pour explorer ces motifs consiste à construire *** des règles d'association *** telles que\n","- **si** acheté *ITEM_1* **alors** achètera *ITEM_2* avec **confiance** *X*.\n","\n","Ces associations n'ont pas à être des règles individuelles. Ils peuvent impliquer de nombreux éléments. Par exemple, une personne dans un supermarché peut ajouter des œufs dans son panier, puis le MBA peut suggérer qu'elle achètera également du pain et/ou de la farine:\n","\n","+ **si**  acheté *OEUFS* **alors** achètera [*PAIN* avec confiance *0,2*; *FARINE* avec confiance 0,05].\n","\n","Cependant, si la personne décide maintenant d'ajouter de la farine à son panier, la nouvelle règle d'association pourrait être comme ci-dessous, suggérant des ingrédients pour faire un gâteau.\n","\n","+ **si** acheté [*OEUFS, FARINE*] **alors** achètera [*SUCRE* avec confiance 0,45; LEVURE avec confiance 0,12; *PAIN* avec confiance *0,03*].\n","\n","Il existe de nombreux scénarios réels où le MBA joue un rôle central dans l'analyse des données, comme les transactions de supermarché, les commandes en ligne ou l'historique des cartes de crédit. Les spécialistes du marketing peuvent utiliser ces règles d'association pour organiser les produits corrélés plus près les uns des autres sur les étagères des magasins ou faire des suggestions en ligne afin que les clients achètent plus d'articles. Un MBA peut généralement aider les détaillants à répondre aux questions les suivantes:\n","\n","- Quels articles sont souvent achetés ensemble ?\n","- Étant donné un panier, quels articles suggérer ?\n","- Comment placer les articles ensemble sur les étagères ?\n","___\n","Market Basket Analysis (MBA) is a data mining analytics technique to uncover associations between products or product grouping. By exploring interesting patterns from an extensive collection of data, MBA aims to understand/reveal customer purchase behaviors based upon the theory that if you purchased a certain set of products, then you are more (or less) likely to buy another group of products. In other words, MBA allows retailers to identify the relationship between the items that customers buy, revealing patterns of items often purchased together.\n","\n","A widely used approach to explore these patterns is by constructing ***association rules*** such as\n","- **if** bought *ITEM_1* **then** will buy *ITEM_2* with **confidence** *X*.\n","\n","These associations do not have to be 1-to-1 rules. They can involve many items. For example, a person in a supermarket may add eggs to his/her cart, then an MBA application may suggest that the person will also buy some bread and/or flour: \n","\n","+ **if** bought *EGGS* **then** will buy [*BREAD* with confidence *0.2*; *FLOUR* with confidence 0.05].\n","\n","However, if the person now decides to add flour to his/her cart, the new association rule could be as showing below, suggesting ingredients to make a cake.\n","\n","+ **if** bought [*EGGS, FLOUR*] **then** will buy [*SUGGAR* with confidence 0.45; BAKING POWDER with confidence 0.12; *BREAD* with confidence *0.03*].\n","\n","There are many real scenarios where MBA plays a central role in data analysis, such as supermarket transactions, online orders or credit card history. Marketers may use these association rules to arrange correlated products closer to each other on store shelves or make online suggestions so that customers buy more items. Some questions that an MBA can usually help retailers to answer are:\n","\n","- What items are often purchased together?\n","- Given a basket, what items should be suggested?\n","- How should items be placed together on the shelves?\n","\n","### Objectif\n","\n","Votre objectif dans ce TP est de développer un algorithme MBA pour révéler les motifs en créant des règles d'association dans un ensemble de données volumineux avec plus de trois millions de transactions de supermarché. Cependant, la collecte de règles d'association dans les grands ensembles de données est un problème très intensif en calcul, ce qui rend presque impossible leur exécution sans système distribué. Par conséquent, pour exécuter votre algorithme, vous aurez accès à un cluster de *cloud computing* distribué avec des centaines de cœurs.\n","\n","À cette fin, un algorithme **MapReduce** sera implémenté avec le framework [Apache Spark](http://spark.apache.org), un système informatique distribué rapide. En résumé, Spark est un framework open source conçu avec une méthodologie *scale-out* qui en fait un outil très puissant pour les programmeurs ou les développeurs d'applications pour effectuer un volume massif de calculs et de traitement de données dans des environnements distribués. Spark fournit des API de haut niveau qui facilitent la création d'applications parallèles sans avoir à se soucier de la façon dont votre code et vos données sont parallélisés / distribués par le cluster informatique. Spark fait tout pour vous.\n","\n","La mise en œuvre suivra l'algorithme d'analyse du panier de marché présenté par Jongwook Woo et Yuhang Xu (2012). L'image **workflow.pdf** illustre le flux de travail de l'algorithme et doit être utilisée pour consultation tout au long de ce TP. Les cases bleues sont celles où vous devez implémenter une méthode pour effectuer une fonction de mappage ou de réduction, et les cases grises représentent leur sortie attendue. **Toutes ces opérations sont expliquées en détail dans les sections suivantes.**\n","___\n","Your goal in this TP is to develop an MBA algorithm for revealing patterns by creating association rules in a big dataset with more than three million supermarket transactions. However, mining association rules for large datasets is a very computationally intensive problem, which makes it almost impractical to perform it without a distributed system. Hence, to run your algorithm, you will have access to a distributed cloud computing cluster with hundreds of cores. \n","\n","To this end, a **MapReduce** algorithm will be implemented upon the [Apache Spark](http://spark.apache.org) framework, a fast cluster computing system. In a nutshell, Spark is an open source framework designed with a *scale-out* methodology which makes it a very powerful tool for programmers or application developers to perform a massive volume of computations and data processing in distributed environments. Spark provides high-level APIs that make it easy to build parallel apps without needing to worry about how your code and data are parallelized/distributed thought the computing cluster. Spark does it all for you.\n","\n","The implementation will follow the Market Basket Analysis algorithm presented by Jongwook Woo and Yuhang Xu (2012). The image **workflow.svg** Illustrates the algorithm's workflow, and is to be used for consultation throughout this TP. The blue boxes are the ones where you must implement a method to perform a map or reduce function, and the gray boxes represent their expected output. **All these operations are explained in detail in the following sections.**\n","\n","## 1. Configuration de Spark\n","\n","Spark fonctionne sur les systèmes Windows et UNIX (par exemple, Linux, Mac OS). Il est facile d'exécuter Spark localement sur une seule machine - tout ce dont vous avez besoin est d'avoir Java installé sur votre système PATH, ou la variable d'environnement JAVA_HOME pointant vers une installation Java. Il est obligatoire que le **JDK v8** soit installé sur votre système, car Spark ne prend actuellement en charge que cette version. Si ce n'est pas le cas, accédez à [la page Web de Java](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) pour télécharger et installer une machine virtuelle Java. N'oubliez pas de définir la variable d'environnement JAVA_HOME pour utiliser JDK v8 si votre installation ne le fait pas automatiquement. Mise à jour 2022: Il est à noté que pour les systèmes d'exploitation de **Google Cloud, JDK v8 n'est pas disponible. Remplacer cela par la JDK 10 (default-jdk) permet également à Spark de fonctionner pour ce TP.**\n","\n","L'interface entre Python et Spark se fait via **PySpark**, qui peut être installé en exécutant `pip install pyspark` ou configuré en suivant la séquence ci-dessous:\n","\n","1. D'abord, allez sur http://spark.apache.org/downloads\n","2. Sélectionnez la dernière version de Spark et le package pré-construit pour Apache Hadoop 2.7\n","3. Cliquez pour télécharger **spark-2.4.5-bin-hadoop2.7.tgz** et décompressez-le dans le dossier de votre choix.\n","4. Ensuite, exportez les variables suivantes pour lier PYSPARK (l'interface python de Spark) à votre distribution python dans votre fichier `~/.bash_profile`.\n","\n","``\n","export SPARK_HOME=/chemin/ vers / spark-2.4.5-bin-hadoop2.7\n","export PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$SPARK_HOME/python/lib/pyspark.zip:$ PYTHONPATH\"\n","export PYSPARK_PYTHON=/chemin/vers/votre/python3\n","``\n","\n","5. Exécutez `source ~./bash_profile` pour effectuer les modifications et redémarrer cette session de notebook jupyter.\n","___\n","Spark runs on both Windows and UNIX-like systems (e.g., Linux, Mac OS). It's easy to run locally on one machine — all you need is to have Java installed on your system PATH, or the JAVA_HOME environment variable pointing to a Java installation. It is mandatory that you have the **JDK v8** installed in your system, as Spark currently only support this version. If you haven't, go to [Java's web page](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html) to download and install a Java Virtual Machine. Remember to set the environment variable JAVA_HOME to use JDK v8 if your installation does not do it automatically for you. **2022 update: on google cloud the operating systems does not support JDK v8. It can be replaced by the JDK v10 (default-jdk) to be able to run the TP.**\n","\n","The interface between Python and Spark is done through **PySpark**, which can be installed by running `pip install pyspark` or set up following the sequence below:\n","\n","1. First, go to http://spark.apache.org/downloads \n","2. Select the newest Spark release and the Pre-built for Apache Hadoop 2.7 package \n","3. Click for download **spark-2.4.5-bin-hadoop2.7.tgz** and unzip it in any folder of your preference. \n","4. Next, export the following variables to link PYSPARK (Spark's python interface) to your python distribution in your `~/.bash_profile` file.\n","\n","``\n","export SPARK_HOME=/path/to/spark-2.4.5-bin-hadoop2.7\n","export PYTHONPATH=\"$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.7-src.zip:$SPARK_HOME/python/lib/pyspark.zip:$PYTHONPATH\"\n","export PYSPARK_PYTHON=/path/to/your/python3\n","``\n","\n","5. Run `source ~./bash_profile` to effectuate the changes and restart this jupyter notebook session.\n","\n","#### Alternative for using Google Collab\n","\n","Si vous pensez utiliser Google Colaboratory, exécutez la cellule de code suivante pour installer Spark\n","___\n","If you are planning on using Google Colaboratory platform, run the following code cell to set up Spark."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11678,"status":"ok","timestamp":1667508458537,"user":{"displayName":"Yoann Monteiro","userId":"02171426768575630386"},"user_tz":240},"id":"JmUMt4htMazm","outputId":"1afa43fd-3cbb-4152-e300-af0ad877c248"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  default-jdk-headless openjdk-11-jdk\n","Suggested packages:\n","  openjdk-11-demo openjdk-11-source visualvm\n","The following NEW packages will be installed:\n","  default-jdk default-jdk-headless openjdk-11-jdk\n","0 upgraded, 3 newly installed, 0 to remove and 4 not upgraded.\n","Need to get 1,564 kB of archives.\n","After this operation, 1,632 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 default-jdk-headless amd64 2:1.11-68ubuntu1~18.04.1 [1,132 B]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openjdk-11-jdk amd64 11.0.16+8-0ubuntu1~18.04 [1,562 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 default-jdk amd64 2:1.11-68ubuntu1~18.04.1 [1,092 B]\n","Fetched 1,564 kB in 1s (1,816 kB/s)\n","Selecting previously unselected package default-jdk-headless.\n","(Reading database ... 124286 files and directories currently installed.)\n","Preparing to unpack .../default-jdk-headless_2%3a1.11-68ubuntu1~18.04.1_amd64.deb ...\n","Unpacking default-jdk-headless (2:1.11-68ubuntu1~18.04.1) ...\n","Selecting previously unselected package openjdk-11-jdk:amd64.\n","Preparing to unpack .../openjdk-11-jdk_11.0.16+8-0ubuntu1~18.04_amd64.deb ...\n","Unpacking openjdk-11-jdk:amd64 (11.0.16+8-0ubuntu1~18.04) ...\n","Selecting previously unselected package default-jdk.\n","Preparing to unpack .../default-jdk_2%3a1.11-68ubuntu1~18.04.1_amd64.deb ...\n","Unpacking default-jdk (2:1.11-68ubuntu1~18.04.1) ...\n","Setting up openjdk-11-jdk:amd64 (11.0.16+8-0ubuntu1~18.04) ...\n","update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n","Setting up default-jdk-headless (2:1.11-68ubuntu1~18.04.1) ...\n","Setting up default-jdk (2:1.11-68ubuntu1~18.04.1) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.3.1)\n","Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.5)\n"]}],"source":["#import os\n","#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","#!pip install pyspark\n","\n","import os\n","!apt install -y default-jdk\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","!pip install pyspark"]},{"cell_type":"markdown","metadata":{"id":"rovSCW_vYs7m"},"source":["#### Testez votre Spark / Test your Spark\n","À l'aide du code suivant, vous pouvez tester si Spark est installé correctement.\n","___\n","Using the following code, you can test if Spark is installed correctly:"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":937},"executionInfo":{"elapsed":581,"status":"error","timestamp":1667508562382,"user":{"displayName":"Yoann Monteiro","userId":"02171426768575630386"},"user_tz":240},"id":"UxgNiBFkYs7n","outputId":"35be9bc8-942c-490a-e82a-219bc1692035"},"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-09d1cd4715b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select 'spark' as hello \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mjsc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mprofiler_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mudf_profiler_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             )\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \"\"\"\n\u001b[1;32m    401\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1586\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2661)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2658)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2748)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:97)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"]}],"source":["import pyspark\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","df = spark.sql(\"select 'spark' as hello \")\n","df.show()"]},{"cell_type":"markdown","metadata":{"id":"QaYpUpOFMazu"},"source":["### 1.1 Exemple de comptage de produits / Products Counting Example \n","\n","Pour tester votre installation et commencer à vous familiariser avec Spark, nous suivrons un exemple qui compte combien de fois les produits d'un toy dataset ont été achetés.\n","\n","Le principal point d'entrée pour commencer la programmation avec Spark est [l'API RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html), une excellente abstraction Spark pour travailler avec MapReduce. RDD est une collection d'éléments partitionnés sur les nœuds du cluster qui peuvent fonctionner en parallèle. En d'autres termes, RDD est la façon dont Spark maintient vos données prêtes à exécuter une fonction (par exemple, une fonction Map ou une fonction reduce) en parallèle. **Ne vous inquiétez pas si cela semble toujours déroutant, il sera clair une fois que vous commencerez à l'implémenter**. Cependant, cela fait partie de ce TP d'étudier / consulter [Spark python API](https://spark.apache.org/docs/latest/api/python/) et d'apprendre à l'utiliser. Certaines fonctions utiles offertes par l'API RDD sont:\n","___\n","To test your installation and start to get familiarized with Spark, we will follow an example that counts how many times the products of a toy dataset were purchased.\n","\n","The main entry point to start programming with Spark is the [RDD API](https://spark.apache.org/docs/latest/rdd-programming-guide.html), an excellent Spark abstraction to work with the MapReduce framework.  RDD is a collection of elements partitioned across the nodes of the cluster that can operate in parallel. In other words, RDD is how Spark keeps your data ready to perform some function (e.g., a map or reduce function) in parallel. **Do not worry if this still sounds confusing, it will be clear once you start implementing**. However, it is part of this TP to study/consult the [Spark python API](https://spark.apache.org/docs/latest/api/python/) and learn how to use it. Some useful functions that the RDD API offers are:\n","\n","1. **map**: return a new RDD by applying a function to each element of this RDD.\n","2. **flatMap**: return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. **Should be used when each entry will yield more than one mapped element**\n","3. **reduce**: reduces the elements of this RDD using the specified commutative and associative binary operator.\n","4. **reduceByKey**: merge the values for each key using an associative and commutative reduce function\n","5. **groupByKey**: group the values for each key in the RDD into a single sequence\n","6. **collect**: return a list that contains all of the elements in this RDD. **Should not be used when working with a lot of data**\n","7. **takeSample**: return a sampled subset of this RDD\n","8. **count**: return the number of elements in this RDD.\n","9. **filter**: return a new RDD containing only the elements that satisfy a predicate."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":171,"status":"error","timestamp":1667508527632,"user":{"displayName":"Yoann Monteiro","userId":"02171426768575630386"},"user_tz":240},"id":"NZDz1nrBMazu","outputId":"1ed87cf2-6f5c-45d0-93ba-41d0bc096f2c","colab":{"base_uri":"https://localhost:8080/","height":937}},"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-20b30dcdadf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Initializates a object of SparkSession class, main entry point to Spark's funcionalites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Read a toy dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mjsc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mprofiler_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                 \u001b[0mudf_profiler_cls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             )\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# Create the Java SparkContext through Py4J\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjsc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;31m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \"\"\"\n\u001b[1;32m    401\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJavaSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1586\u001b[0;31m             answer, self._gateway_client, None, self._fqn)\n\u001b[0m\u001b[1;32m   1587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:750)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2661)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2658)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2748)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:97)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","def map_to_product(row):\n","    \"\"\"\n","    Map each transaction into a set of KEY-VALUE elements.\n","    The KEY is the word (product) itself and the VALUE is its number of apparitions.\n","    \"\"\"\n","    products = row.transaction.split(';') # split products from the column transaction\n","    for p in products:\n","        yield (p, 1)\n","\n","def reduce_product_by_key(value1, value2):\n","    \"Reduce the mapped objects to unique words by merging (summing ) their values\"\n","    return value1+value2\n","\n","# Initializates a object of SparkSession class, main entry point to Spark's funcionalites\n","spark = SparkSession.builder.getOrCreate()\n","        \n","# Read a toy dataset\n","toy = spark.read.csv('instacart/toy.csv', header=True)\n","print(\"Toy dataset\")\n","toy.show()\n","# Obtain a RDD object to call a map function\n","toy_rdd = toy.rdd\n","print(\"Toy dataframe as a RDD object (list of Row objects):\\n\\t\", toy_rdd.collect())\n","\n","# Map function to identify all products\n","toy_rdd = toy_rdd.flatMap(map_to_product)\n","print(\"\\nMapped products:\\n\\t\", toy_rdd.collect())\n","\n","# Reduce function to merge values of elements that share the same KEY\n","toy_rdd = toy_rdd.reduceByKey(reduce_product_by_key)\n","print(\"\\nReduced (merged) products:\\n\\t\", toy_rdd.collect())\n","\n","print(\"\\nVisualizing as a dataframe:\")\n","toy_rdd.toDF([\"product\", \"count_product\"]).show()"]},{"cell_type":"markdown","metadata":{"id":"PpJGQmzXMazz"},"source":["### 1.2 Travailler avec Spark Dataframe / Working with Spark Dataframes\n","\n","Dans l'exemple ci-dessus, nous avons brièvement utilisé une classe Dataframe de Spark, mais uniquement pour obtenir un objet RDD avec ``toy.rdd`` et pour aficher les données sous forme de tableau structuré avec le ``show ()`` une fonction. Cependant, [Dataframe](http://spark.apache.org/docs/latest/api/python/) est une partie cruciale de la version actuelle de Spark et est construit sur l'API RDD. Il s'agit d'une collection distribuée de lignes sous des colonnes nommées, identique à une table dans une base de données relationnelle. Le Dataframe de Spark fonctionne de la même manière que [Pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). En fait, nous pouvons exporter (obtenir) une Dataframe Spark vers (à partir de) ​​une Dataframe pandas avec la fonction ``toPandas()``  (``spark.createDataFrame``).\n","\n","Une fonctionnalité centrale du Dataframe est de bénéficier du [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html#sql), un module qui permet des requêtes SQL sur des données structurées. Par exemple, le même « exemple de comptage de produits » aurait pu être implémenté comme une séquence d'opérations SQL sur les données:\n","___\n","\n","In the example above, we briefly used a Spark's Dataframe class, but only to obtain an RDD object with ```toy.rdd``` and to print the data as a structured table with the ```show()``` function. However, [Dataframe](http://spark.apache.org/docs/latest/api/python/) is a crucial part of the current Spark release and is built upon the RDD API. It is a distributed collection of rows under named columns, the same as a table in a relational database. Spark's Dataframe works similarly as [Pandas'](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). In fact, we can export (obtain) a Spark's dataframe to (from) a pandas' data frame with the function ```toPandas()``` (```spark.createDataFrame```).\n","\n","A central functionality of the data frame is to profit from the [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html#sql), a module that allows SQL queries over structured data. For example, the same 'product counting example' could have been implemented as a sequence of SQL operations over the data:  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2970,"status":"ok","timestamp":1664683815361,"user":{"displayName":"Yoann Monteiro","userId":"02171426768575630386"},"user_tz":240},"id":"oFL6BuIDMaz0","outputId":"c0c97b19-e1d0-4384-cdcc-bbb482df30d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["New column 'products': exploding the transaction's products to a new row\n","+--------+-----------+--------+\n","|order_id|transaction|products|\n","+--------+-----------+--------+\n","|       1|    a;b;c;f|       a|\n","|       1|    a;b;c;f|       b|\n","|       1|    a;b;c;f|       c|\n","|       1|    a;b;c;f|       f|\n","|       2|    d;b;a;e|       d|\n","|       2|    d;b;a;e|       b|\n","|       2|    d;b;a;e|       a|\n","|       2|    d;b;a;e|       e|\n","|       3|        c;b|       c|\n","|       3|        c;b|       b|\n","|       4|        b;c|       b|\n","|       4|        b;c|       c|\n","+--------+-----------+--------+\n","\n","Couting unique products:\n","+--------+-------------+\n","|products|count_product|\n","+--------+-------------+\n","|       b|            4|\n","|       c|            3|\n","|       a|            2|\n","|       f|            1|\n","|       e|            1|\n","|       d|            1|\n","+--------+-------------+\n","\n"]}],"source":["import pyspark.sql.functions as f\n","\n","# Creates a new column, products, with all products appering in each transaction\n","print('New column \\'products\\': exploding the transaction\\'s products to a new row')\n","df_toy = toy.withColumn('products', f.explode(f.split(toy.transaction, ';')))\n","df_toy.show()\n","\n","# Performs a select query and group rows by the product name, aggreagating by counting\n","print('Couting unique products:')\n","df_toy.select(df_toy.products)\\\n","      .groupBy(df_toy.products)\\\n","      .agg(f.count('products').alias('count_product'))\\\n","      .sort('count_product', ascending=False)\\\n","      .show()"]},{"cell_type":"markdown","metadata":{"id":"W4HFs8CVMaz3"},"source":["En outre, les mêmes opérations SQL effectuées ci-dessus auraient pu être effectuées avec une requête en langage SQL traditionnel comme indiqué ci-dessous:\n","___\n","Also, the same SQL operations performed above could have been done with a traditional SQL language query as showing below:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":761,"status":"ok","timestamp":1664683816119,"user":{"displayName":"Yoann Monteiro","userId":"02171426768575630386"},"user_tz":240},"id":"O_eYl-7tMaz3","outputId":"067ebbe8-df8d-405a-e338-a8255724742e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+-------------+\n","|products|product_count|\n","+--------+-------------+\n","|       b|            4|\n","|       c|            3|\n","|       a|            2|\n","|       f|            1|\n","|       e|            1|\n","|       d|            1|\n","+--------+-------------+\n","\n"]}],"source":["# Creates a relational table TOY in the Spark session\n","df_toy.createOrReplaceTempView(\"TOY\")\n","\n","spark.sql(\"SELECT t.products, COUNT(t.products) AS product_count\"\n","          \" FROM TOY t\"\n","          \" GROUP BY t.products\"\n","          \" ORDER BY product_count DESC\").show()"]},{"cell_type":"markdown","metadata":{"id":"MTMT2RlqQBj9"},"source":["Ces concepts SQL sont mentionnés ici car ils nous seront utiles lors du TP, principalement dans la section 3, pour manipuler les données du supermarché, qui sont structurées en tableaux. Ainsi, si vous n'êtes pas familier avec SQL, il est recommandé de suivre un [tutoriel](https://www.w3schools.com/sql/) pour comprendre les bases.\n","___\n","These SQL concepts are being mentioned here because they will be useful to us during the TP, mainly in Section 3, to manipulate the supermarket data, which is structured in tables. Thus, if you are not familiar with SQL, it is recommended that you follow a [tutorial](https://www.w3schools.com/sql/) to understand the basics."]},{"cell_type":"markdown","metadata":{"id":"s1oZzC-ZQEE0"},"source":["\n","\n","## 2. Algorithme MBA (45 points)\n","Les sections suivantes expliquent comment développer chaque étape de l'algorithme MapReduce pour notre application de supermarché. La figure workflow.pdf illustre chaque étape de l'algorithme.\n","___\n","The following sections explain how you should develop each step of the MapReduce algorithm for our supermarket application. Figure workflow.png illustrates each step of the algorithm."]},{"cell_type":"markdown","metadata":{"id":"T0Onr2NxMaz8"},"source":["\n","\n","### 2.1 Map to Patterns / Map to Patterns (10 points)\n","Pour un sous-ensemble de transactions (c'est-à-dire les lignes de notre toy dataset), chaque transaction doit être **mappée** vers un ensemble de *motifs d'achat* trouvés dans la transaction. Formellement, ces motifs sont des sous-ensembles de produits qui représentent un groupe d'articles achetés ensemble. \n","\n","Pour le framework MapReduce, chaque motif doit être créé comme un élément *KEY-VALUE*, où la KEY peut prendre la forme d'un singleton, d'une paire ou d'un trio de produits présents dans la transaction. Plus précisément, pour chaque transaction, la fonction de mappage doit générer tous les sous-ensembles **UNIQUE** possibles de taille **UN, DEUX ou TROIS**. La VALEUR associée à chaque KEY est le nombre de fois que la KEY est apparue dans la transaction (si nous supposons qu'aucun produit n'apparaît plus d'une fois dans la transaction, cette valeur est toujours égale à un).\n","\n","Maintenant, implémentez la fonction **map_to_patterns** qui reçoit une transaction (une ligne du dataset) et retourne les motifs trouvés dans la transaction. Les éléments mappés sont un tuple (KEY, VALUE), où KEY est également un tuple de noms de produits. Il est crucial de noter que, puisque chaque entrée (transaction) de la fonction MAP produira **plus** un élément KEY-VALUE, un *flatMap* doit être invoqué pour cette étape.\n","\n","Pour le toy dataset, la sortie attendue est similaire à:\n","___\n","For a given set of transactions (i.e., the rows of our toy dataset), each transaction must be **mapped** into a set of *purchase patterns* found within the transaction. Formally, these patterns are subsets of products that represent a group of items bought together. \n","\n","For the MapReduce framework, each pattern must be created as a *KEY-VALUE* element, where the KEY can take the form of a singleton, a pair or a trio of products that are present in the transaction. More precisely, for each transaction, the mapping function must generate all possible **UNIQUE** subsets of size **ONE, TWO or THREE**.  The VALUE associated with each KEY is the number of times that the KEY appeared in the transaction (if we assume that no product appears more than once in the transaction, this value is always equal to one). \n","\n","Now, implement the **map_to_patterns** function that receives a transaction (a row from the data frame) and returns the patterns found in the transaction. The mapped elements are a tuple (KEY, VALUE), where KEY is also a tuple of product names. It is crucial to notice that, since each entry (transaction) of the map function will **yield** more than one KEY-VALUE element, a *flatMap* must be invoked for this step.\n","\n","For the toy dataset, the expected output is similar to:\n","\n","\n","\n","<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 20em; padding-left:1px\">\n","<code>\n","+---------------+-----------+\n","|       patterns|occurrences|\n","+---------------+-----------+\n","|         ('a',)|          1|\n","|     ('a', 'b')|          1|\n","|('a', 'b', 'c')|          1|\n","|('a', 'b', 'f')|          1|\n","|     ('a', 'c')|          1|\n","|('a', 'c', 'f')|          1|\n","|     ('a', 'f')|          1|\n","|         ('b',)|          1|\n","|     ('b', 'c')|          1|\n","|('b', 'c', 'f')|          1|\n","|     ('b', 'f')|          1|\n","|         ('c',)|          1|\n","|     ('c', 'f')|          1|\n","|         ('f',)|          1|\n","|         ('a',)|          1|\n","|     ('a', 'b')|          1|\n","|('a', 'b', 'd')|          1|\n","|('a', 'b', 'e')|          1|\n","|     ('a', 'd')|          1|\n","|('a', 'd', 'e')|          1|\n","|     ('a', 'e')|          1|\n","|         ('b',)|          1|\n","|     ('b', 'd')|          1|\n","|('b', 'd', 'e')|          1|\n","|     ('b', 'e')|          1|\n","|         ('d',)|          1|\n","|     ('d', 'e')|          1|\n","|         ('e',)|          1|\n","|         ('b',)|          1|\n","|     ('b', 'c')|          1|\n","|         ('c',)|          1|\n","|         ('b',)|          1|\n","|     ('b', 'c')|          1|\n","|         ('c',)|          1|\n","+---------------+-----------+\n","</code>\n","</pre>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1664683816120,"user":{"displayName":"Yoann Monteiro","userId":"02171426768575630386"},"user_tz":240},"id":"4AU-h6mb_fqW","outputId":"4eedb605-8bad-4c7f-c6ec-e75d3ef40c3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["('a', 'b', 'c')\n","('a', 'b', 'd')\n","('a', 'c', 'd')\n","('b', 'c', 'd')\n"]}],"source":["#Test des combinaisons\n","import itertools\n","\n","toys = ['b','c','a','d']\n","toys.sort()\n","combinations = list(itertools.combinations(toys,3))\n","for element in combinations:\n","  print(element)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":345,"status":"ok","timestamp":1664683816462,"user":{"displayName":"Yoann Monteiro","userId":"02171426768575630386"},"user_tz":240},"id":"FVSdOuajUQXb","outputId":"a968b07b-aaa7-4414-c26f-e5bb922a1b2e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------+-----------+\n","|       patterns|occurrences|\n","+---------------+-----------+\n","|         ('a',)|          1|\n","|         ('b',)|          1|\n","|         ('c',)|          1|\n","|         ('f',)|          1|\n","|     ('a', 'b')|          1|\n","|     ('a', 'c')|          1|\n","|     ('a', 'f')|          1|\n","|     ('b', 'c')|          1|\n","|     ('b', 'f')|          1|\n","|     ('c', 'f')|          1|\n","|('a', 'b', 'c')|          1|\n","|('a', 'b', 'f')|          1|\n","|('a', 'c', 'f')|          1|\n","|('b', 'c', 'f')|          1|\n","|         ('a',)|          1|\n","|         ('b',)|          1|\n","|         ('d',)|          1|\n","|         ('e',)|          1|\n","|     ('a', 'b')|          1|\n","|     ('a', 'd')|          1|\n","|     ('a', 'e')|          1|\n","|     ('b', 'd')|          1|\n","|     ('b', 'e')|          1|\n","|     ('d', 'e')|          1|\n","|('a', 'b', 'd')|          1|\n","|('a', 'b', 'e')|          1|\n","|('a', 'd', 'e')|          1|\n","|('b', 'd', 'e')|          1|\n","|         ('b',)|          1|\n","|         ('c',)|          1|\n","|     ('b', 'c')|          1|\n","|         ('b',)|          1|\n","|         ('c',)|          1|\n","|     ('b', 'c')|          1|\n","+---------------+-----------+\n","\n"]}],"source":["import itertools\n","def format_tuples(pattern):\n","    \"\"\"\n","    Used for visualizition.\n","    Transforms tuples to a string since Dataframe does not support column of tuples with different sizes\n","    (a,b,c) -> '(a,b,c)'\n","    \"\"\"\n","    return (str(pattern[0]), str(pattern[1]))\n","\n","def map_to_patterns(row):\n","    products_list = row.transaction.split(\";\")\n","    products_list.sort()\n","    for i in range(1,4): \n","      patterns = list(itertools.combinations(products_list,i))\n","      for pattern in patterns:\n","        yield(pattern,1)\n","\n","toy_rdd = toy.rdd\n","patterns_rdd = toy_rdd.flatMap(map_to_patterns)\n","# Output as dataframe\n","patterns_rdd.map(format_tuples).toDF(['patterns', 'occurrences']).show(50)"]},{"cell_type":"markdown","metadata":{"id":"YvvRw0plMa0B"},"source":["### 2.2  Reduce patterns  (2,5 points)\n","Une fois que différents processeurs ont traité les transactions, une fonction **reduce** doit être appelée pour combiner des KEYS identiques (le sous-ensemble de produits) et calculer le nombre total de ses occurrences dans le dataset. En d'autres termes, cette procédure de réduction doit additionner la *VALUE* de chaque KEY identique.\n","\n","Créez ci-dessous une fonction **reduce_patterns** qui doit additionner la VALUE de chaque motif.\n","Pour le toy dataset, la sortie attendue est:\n","___\n","Once different CPUs processed the transactions, a **reduce** function must take place to combine identical KEYS (the subset of products) and compute the total number of its occurrences in the entire dataset. In other words, this reduce procedure must sum the *VALUE* of each identical KEY.\n","\n","Create a **reduce_patterns** function below that must sum the VALUE of each pattern.\n","For the toy dataset, the expected output is:\n","\n","<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 28em; padding-left:5px\">\n","<code>\n","+---------------+--------------------+\n","|       patterns|combined_occurrences|\n","+---------------+--------------------+\n","|         ('a',)|                   2|\n","|     ('a', 'b')|                   2|\n","|('a', 'b', 'c')|                   1|\n","|('a', 'b', 'f')|                   1|\n","|     ('a', 'c')|                   1|\n","|('a', 'c', 'f')|                   1|\n","|     ('a', 'f')|                   1|\n","|         ('b',)|                   4|\n","|     ('b', 'c')|                   3|\n","|('b', 'c', 'f')|                   1|\n","|     ('b', 'f')|                   1|\n","|         ('c',)|                   3|\n","|     ('c', 'f')|                   1|\n","|         ('f',)|                   1|\n","|('a', 'b', 'd')|                   1|\n","|('a', 'b', 'e')|                   1|\n","|     ('a', 'd')|                   1|\n","|('a', 'd', 'e')|                   1|\n","|     ('a', 'e')|                   1|\n","|     ('b', 'd')|                   1|\n","|('b', 'd', 'e')|                   1|\n","|     ('b', 'e')|                   1|\n","|         ('d',)|                   1|\n","|     ('d', 'e')|                   1|\n","|         ('e',)|                   1|\n","+---------------+--------------------+\n","</code>\n","</pre>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":550,"status":"ok","timestamp":1664683817011,"user":{"displayName":"Yoann Monteiro","userId":"02171426768575630386"},"user_tz":240},"id":"67IKY_4MMa0C","outputId":"4aabc1dc-5643-4512-abd2-56c3f2ffc0cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------+--------------------+\n","|       patterns|combined_occurrences|\n","+---------------+--------------------+\n","|         ('a',)|                   2|\n","|         ('b',)|                   4|\n","|         ('c',)|                   3|\n","|         ('f',)|                   1|\n","|     ('a', 'b')|                   2|\n","|     ('a', 'c')|                   1|\n","|     ('a', 'f')|                   1|\n","|     ('b', 'c')|                   3|\n","|     ('b', 'f')|                   1|\n","|     ('c', 'f')|                   1|\n","|('a', 'b', 'c')|                   1|\n","|('a', 'b', 'f')|                   1|\n","|('a', 'c', 'f')|                   1|\n","|('b', 'c', 'f')|                   1|\n","|         ('d',)|                   1|\n","|         ('e',)|                   1|\n","|     ('a', 'd')|                   1|\n","|     ('a', 'e')|                   1|\n","|     ('b', 'd')|                   1|\n","|     ('b', 'e')|                   1|\n","+---------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["def reduce_patterns(occurences1,occurrences2):\n","  return occurences1+occurrences2\n","\n","combined_patterns_rdd = patterns_rdd.reduceByKey(reduce_patterns)\n","\n","# Output as dataframe\n","combined_patterns_rdd.map(format_tuples).toDF(['patterns', 'combined_occurrences']).show()"]},{"cell_type":"markdown","metadata":{"id":"6BME1VugMa0F"},"source":["### 2.3 Map to subpatterns (15 points)\n","Ensuite, une autre fonction **map** doit être appliquée pour générer des sous-motifs. Encore une fois, les sous-motifs sont des éléments KEY-VALUE, où la KEY est également un sous-ensemble de produits. Cependant, la création de la KEY du sous-motif est une procédure différente. Cette fois, l'idée est de décomposer la liste des produits de chaque motif (KEY), de supprimer un produit à la fois et de produire la liste résultante en tant que nouvelle clé de sous-motif.\n","\n","Par exemple, pour un modèle donné $P$ avec trois produits, $p_1, p_2$ et $p_3$, trois nouvelles clés de sous-motifs vont être créées: (i) supprimer $p_1$ et retourner ($p_2, p_3$) ; (ii) supprimer $p_2$ et retourner ($p_1, p_3$); et (iii) supprimer $p_3$ et retourner ($p_1, p_2$).\n","\n","De plus, la structure VALUE du sous-motif sera également différente. Au lieu d'une seule valeur entière unique comme nous l'avons eu dans les motifs, cette fois un *tuple* devrait être créé pour le sous-motif VALUE. Ce tuple contient le produit qui a été retiré lors de la remise de la KEY et le nombre de fois que le motif est apparu. Par exemple ci-dessus, les valeurs doivent être ($p_1,v$), ($p_2,v$) et ($p_3,v $), respectivement, où $v$ est la VALEUR du motif.\n","\n","L'idée derrière les sous-motif est de créer **des règles** telles que : lorsque les produits de KEY ont été achetés, l'article présent dans la VALEUR a également été acheté *v* fois. En outre, chaque motif doit également produire un sous-motif dans lequel la clé est la même liste de produits du motif, mais la valeur est un tuple avec un produit nul (None) et le nombre de fois que le motif est apparu. Cet élément sera utile pour garder une trace du nombre de fois où un tel motif a été trouvé et sera utilisé ultérieurement pour calculer la valeur de confiance lors de la génération des règles d'association.\n","\n","Maintenant, implémentez la fonction **map_to_subpatterns** qui reçoit un motif et produit tous les sous-motif trouvés. Encore une fois, chaque entrée (motif) générera plus d'un élément KEY-VALUE, puis une fonction flatMap doit être appelée.\n","\n","Pour le toy dataset, la sortie attendue est:\n","___\n","Next, another **map** function should be applied to generate subpatterns. Once again, the subpatterns are KEY-VALUE elements, where the KEY is a subset of products as well. However, creating the subpattern's KEY is a different procedure. This time, the idea is to break down the list of products of each pattern (pattern KEY), remove one product at a time, and yield the resulting list as the new subpattern KEY. \n","\n","For example, for a given pattern $P$ with three products, $p_1, p_2 $ and $p_3$, three new subpatterns KEYs are going to be created: (i) remove $p_1$ and yield ($p_2, p_3$); (ii) remove $p_2$ and yield ($p_1,p_3$); and (iii) remove $p_3$ and yield ($p_1,p_2$). \n","\n","Additionally, the subpattern's VALUE structure will also be different. Instead of just single integer value as we had in the patterns, this time a *tuple* should be created for the subpattern VALUE. This tuple contains the product that was removed when yielding the KEY and the number of times the pattern appeared. For example above, the values should be ($p_1,v$), ($p_2,v$) and ($p_3,v$), respectively, where $v$ is the VALUE of the pattern. \n","\n","The idea behind subpatterns is to create **rules** such as: when the products of KEY were bought, the item present in the VALUE was also bought *v* times. Furthermore, each pattern should also yield a subpattern where the KEY is the same list of products of the pattern, but the VALUE is a tuple with a null product (None) and the number of times the pattern appeared. This element will be useful to keep track of how many times such a pattern was found and later will be used to compute the confidence value when generating the association rules. \n","\n","Now, implement the  **map_to_subpatterns** function that receives a pattern and yields all found subpatterns. Once again, each entry (pattern) will generate more than one KEY-VALUE element, then a flatMap function must be called.\n","\n","For the toy dataset, the expected output is:\n","\n","<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 20em; padding-left:5px\">\n","<code>\n","+---------------+---------+\n","|    subpatterns|    rules|\n","+---------------+---------+\n","|         ('a',)|(None, 2)|\n","|     ('a', 'b')|(None, 2)|\n","|         ('b',)| ('a', 2)|\n","|         ('a',)| ('b', 2)|\n","|('a', 'b', 'c')|(None, 1)|\n","|     ('b', 'c')| ('a', 1)|\n","|     ('a', 'c')| ('b', 1)|\n","|     ('a', 'b')| ('c', 1)|\n","|('a', 'b', 'f')|(None, 1)|\n","|     ('b', 'f')| ('a', 1)|\n","|     ('a', 'f')| ('b', 1)|\n","|     ('a', 'b')| ('f', 1)|\n","|     ('a', 'c')|(None, 1)|\n","|         ('c',)| ('a', 1)|\n","|         ('a',)| ('c', 1)|\n","|('a', 'c', 'f')|(None, 1)|\n","|     ('c', 'f')| ('a', 1)|\n","|     ('a', 'f')| ('c', 1)|\n","|     ('a', 'c')| ('f', 1)|\n","|     ('a', 'f')|(None, 1)|\n","|         ('f',)| ('a', 1)|\n","|         ('a',)| ('f', 1)|\n","|         ('b',)|(None, 4)|\n","|     ('b', 'c')|(None, 3)|\n","|         ('c',)| ('b', 3)|\n","|         ('b',)| ('c', 3)|\n","|('b', 'c', 'f')|(None, 1)|\n","|     ('c', 'f')| ('b', 1)|\n","|     ('b', 'f')| ('c', 1)|\n","|     ('b', 'c')| ('f', 1)|\n","|     ('b', 'f')|(None, 1)|\n","|         ('f',)| ('b', 1)|\n","|         ('b',)| ('f', 1)|\n","|         ('c',)|(None, 3)|\n","|     ('c', 'f')|(None, 1)|\n","|         ('f',)| ('c', 1)|\n","|         ('c',)| ('f', 1)|\n","|         ('f',)|(None, 1)|\n","|('a', 'b', 'd')|(None, 1)|\n","|     ('b', 'd')| ('a', 1)|\n","|     ('a', 'd')| ('b', 1)|\n","|     ('a', 'b')| ('d', 1)|\n","|('a', 'b', 'e')|(None, 1)|\n","|     ('b', 'e')| ('a', 1)|\n","|     ('a', 'e')| ('b', 1)|\n","|     ('a', 'b')| ('e', 1)|\n","|     ('a', 'd')|(None, 1)|\n","|         ('d',)| ('a', 1)|\n","|         ('a',)| ('d', 1)|\n","|('a', 'd', 'e')|(None, 1)|\n","|     ('d', 'e')| ('a', 1)|\n","|     ('a', 'e')| ('d', 1)|\n","|     ('a', 'd')| ('e', 1)|\n","|     ('a', 'e')|(None, 1)|\n","|         ('e',)| ('a', 1)|\n","|         ('a',)| ('e', 1)|\n","|     ('b', 'd')|(None, 1)|\n","|         ('d',)| ('b', 1)|\n","|         ('b',)| ('d', 1)|\n","|('b', 'd', 'e')|(None, 1)|\n","|     ('d', 'e')| ('b', 1)|\n","|     ('b', 'e')| ('d', 1)|\n","|     ('b', 'd')| ('e', 1)|\n","|     ('b', 'e')|(None, 1)|\n","|         ('e',)| ('b', 1)|\n","|         ('b',)| ('e', 1)|\n","|         ('d',)|(None, 1)|\n","|     ('d', 'e')|(None, 1)|\n","|         ('e',)| ('d', 1)|\n","|         ('d',)| ('e', 1)|\n","|         ('e',)|(None, 1)|\n","+---------------+---------+\n","</code>\n","</pre>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GLZpaDdJIG7C","outputId":"a2593707-676b-48a0-bdc0-eb4fc2d94e7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pattern initial : (('a', 'b', 'c'), 4)\n","Un des sub patterns : (('b', 'c'), ('a', 4))\n"]}],"source":["#Test de suppression d'un élément d'un pattern\n","pattern = (('a','b','c'),4)\n","key,value = pattern\n","key_list = list(key)\n","key_list.remove('a')\n","sub_pattern = tuple(key_list)\n","print(\"Pattern initial : \" + str(pattern))\n","print(\"Un des sub patterns : \" + str((sub_pattern,('a',value))))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1664683817385,"user":{"displayName":"Yoann Monteiro","userId":"02171426768575630386"},"user_tz":240},"id":"t8aLrdMuMa0G","outputId":"224d0348-5099-43b6-b79b-e87571041667"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------+---------+\n","|    subpatterns|    rules|\n","+---------------+---------+\n","|         ('a',)|(None, 2)|\n","|         ('b',)|(None, 4)|\n","|         ('c',)|(None, 3)|\n","|         ('f',)|(None, 1)|\n","|         ('b',)| ('a', 2)|\n","|         ('a',)| ('b', 2)|\n","|     ('a', 'b')|(None, 2)|\n","|         ('c',)| ('a', 1)|\n","|         ('a',)| ('c', 1)|\n","|     ('a', 'c')|(None, 1)|\n","|         ('f',)| ('a', 1)|\n","|         ('a',)| ('f', 1)|\n","|     ('a', 'f')|(None, 1)|\n","|         ('c',)| ('b', 3)|\n","|         ('b',)| ('c', 3)|\n","|     ('b', 'c')|(None, 3)|\n","|         ('f',)| ('b', 1)|\n","|         ('b',)| ('f', 1)|\n","|     ('b', 'f')|(None, 1)|\n","|         ('f',)| ('c', 1)|\n","|         ('c',)| ('f', 1)|\n","|     ('c', 'f')|(None, 1)|\n","|     ('b', 'c')| ('a', 1)|\n","|     ('a', 'c')| ('b', 1)|\n","|     ('a', 'b')| ('c', 1)|\n","|('a', 'b', 'c')|(None, 1)|\n","|     ('b', 'f')| ('a', 1)|\n","|     ('a', 'f')| ('b', 1)|\n","|     ('a', 'b')| ('f', 1)|\n","|('a', 'b', 'f')|(None, 1)|\n","|     ('c', 'f')| ('a', 1)|\n","|     ('a', 'f')| ('c', 1)|\n","|     ('a', 'c')| ('f', 1)|\n","|('a', 'c', 'f')|(None, 1)|\n","|     ('c', 'f')| ('b', 1)|\n","|     ('b', 'f')| ('c', 1)|\n","|     ('b', 'c')| ('f', 1)|\n","|('b', 'c', 'f')|(None, 1)|\n","|         ('d',)|(None, 1)|\n","|         ('e',)|(None, 1)|\n","|         ('d',)| ('a', 1)|\n","|         ('a',)| ('d', 1)|\n","|     ('a', 'd')|(None, 1)|\n","|         ('e',)| ('a', 1)|\n","|         ('a',)| ('e', 1)|\n","|     ('a', 'e')|(None, 1)|\n","|         ('d',)| ('b', 1)|\n","|         ('b',)| ('d', 1)|\n","|     ('b', 'd')|(None, 1)|\n","|         ('e',)| ('b', 1)|\n","|         ('b',)| ('e', 1)|\n","|     ('b', 'e')|(None, 1)|\n","|         ('e',)| ('d', 1)|\n","|         ('d',)| ('e', 1)|\n","|     ('d', 'e')|(None, 1)|\n","|     ('b', 'd')| ('a', 1)|\n","|     ('a', 'd')| ('b', 1)|\n","|     ('a', 'b')| ('d', 1)|\n","|('a', 'b', 'd')|(None, 1)|\n","|     ('b', 'e')| ('a', 1)|\n","|     ('a', 'e')| ('b', 1)|\n","|     ('a', 'b')| ('e', 1)|\n","|('a', 'b', 'e')|(None, 1)|\n","|     ('d', 'e')| ('a', 1)|\n","|     ('a', 'e')| ('d', 1)|\n","|     ('a', 'd')| ('e', 1)|\n","|('a', 'd', 'e')|(None, 1)|\n","|     ('d', 'e')| ('b', 1)|\n","|     ('b', 'e')| ('d', 1)|\n","|     ('b', 'd')| ('e', 1)|\n","|('b', 'd', 'e')|(None, 1)|\n","+---------------+---------+\n","\n"]}],"source":["from copy import deepcopy\n","#erreur \n","def map_to_subpatterns(pattern):\n","  key, value = pattern\n","\n","  #Si le pattern contient plus d'un élément,...\n","  if (len(key)>1):\n","    #....on retire un élement puis on renvoie le sub pattern créé\n","    for toy in key:\n","      key_list = list(key)\n","      key_list.remove(toy)\n","      sub_pattern = tuple(key_list)\n","      yield(sub_pattern,(toy,value))\n","    \n","    #On affiche également le pattern inital\n","    yield(key,(None,value))\n","    \n","  #Sinon, on affiche None\n","  else:\n","    yield(key,(None,value))\n","subpatterns_rdd = combined_patterns_rdd.flatMap(map_to_subpatterns)\n","\n","# Output as dataframe\n","subpatterns_rdd.map(format_tuples).toDF(['subpatterns', 'rules']).show(100)"]},{"cell_type":"markdown","metadata":{"id":"jl6TWh8rMa0J"},"source":["### 2.4 Reduce Subpatterns (2.5 points)\n","\n","Encore une fois, une fonction **reduce** est nécessaire pour regrouper tous les sous-motif par leur KEY. L'objectif de cette procédure de réduction est de créer une liste de toutes les **règles** apparues dans KEY. Par conséquent, la sortie attendue résultant de cette fonction de réduction est également un élément KEY-VALUE, où la clé est la KEY du sous-motif et la valeur est un groupe contenant toutes les valeurs des sous-motif qui partagent la même clé.\n","\n","Pour le toy dataset, la sortie attendue est:\n","___\n","Once again, a **reduce** function will be required to group all the subpatterns by their KEY. The objective of this reducing procedure is to create a list of all **rules** that appeared in KEY. Hence, the expected output resulting from this reduce function is also a KEY-VALUE element, where the KEY is the subpattern's KEY, and the VALUE is a group containing all the VALUEs of the subpatterns that share the same KEY.\n","\n","For the toy dataset, the expected output is:\n","\n","\n","<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 50em; padding-left:5px\">\n","<code>\n","+---------------+-------------------------------------------------------------+\n","|subpatterns    |combined_rules                                               |\n","+---------------+-------------------------------------------------------------+\n","|('a',)         |[(None, 2), ('b', 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]|\n","|('a', 'b')     |[(None, 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]          |\n","|('b',)         |[('a', 2), (None, 4), ('c', 3), ('f', 1), ('d', 1), ('e', 1)]|\n","|('a', 'b', 'c')|[(None, 1)]                                                  |\n","|('b', 'c')     |[('a', 1), (None, 3), ('f', 1)]                              |\n","|('a', 'c')     |[('b', 1), (None, 1), ('f', 1)]                              |\n","|('a', 'b', 'f')|[(None, 1)]                                                  |\n","|('b', 'f')     |[('a', 1), ('c', 1), (None, 1)]                              |\n","|('a', 'f')     |[('b', 1), ('c', 1), (None, 1)]                              |\n","|('c',)         |[('a', 1), ('b', 3), (None, 3), ('f', 1)]                    |\n","|('a', 'c', 'f')|[(None, 1)]                                                  |\n","|('c', 'f')     |[('a', 1), ('b', 1), (None, 1)]                              |\n","|('f',)         |[('a', 1), ('b', 1), ('c', 1), (None, 1)]                    |\n","|('b', 'c', 'f')|[(None, 1)]                                                  |\n","|('a', 'b', 'd')|[(None, 1)]                                                  |\n","|('b', 'd')     |[('a', 1), (None, 1), ('e', 1)]                              |\n","|('a', 'd')     |[('b', 1), (None, 1), ('e', 1)]                              |\n","|('a', 'b', 'e')|[(None, 1)]                                                  |\n","|('b', 'e')     |[('a', 1), ('d', 1), (None, 1)]                              |\n","|('a', 'e')     |[('b', 1), ('d', 1), (None, 1)]                              |\n","+---------------+-------------------------------------------------------------+\n","</code>\n","</pre>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":497,"status":"ok","timestamp":1664683817880,"user":{"displayName":"Yoann Monteiro","userId":"02171426768575630386"},"user_tz":240},"id":"LOP-SVIhMa0J","outputId":"4c1ab1ad-bb9a-40e0-de70-76c8a9d85550"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------+-------------------------------------------------------------+\n","|subpatterns    |combined_rules                                               |\n","+---------------+-------------------------------------------------------------+\n","|('a',)         |[(None, 2), ('b', 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]|\n","|('b',)         |[(None, 4), ('a', 2), ('c', 3), ('f', 1), ('d', 1), ('e', 1)]|\n","|('c',)         |[(None, 3), ('a', 1), ('b', 3), ('f', 1)]                    |\n","|('f',)         |[(None, 1), ('a', 1), ('b', 1), ('c', 1)]                    |\n","|('a', 'b')     |[(None, 2), ('c', 1), ('f', 1), ('d', 1), ('e', 1)]          |\n","|('a', 'c')     |[(None, 1), ('b', 1), ('f', 1)]                              |\n","|('a', 'f')     |[(None, 1), ('b', 1), ('c', 1)]                              |\n","|('b', 'c')     |[(None, 3), ('a', 1), ('f', 1)]                              |\n","|('b', 'f')     |[(None, 1), ('a', 1), ('c', 1)]                              |\n","|('c', 'f')     |[(None, 1), ('a', 1), ('b', 1)]                              |\n","|('a', 'b', 'c')|[(None, 1)]                                                  |\n","|('a', 'b', 'f')|[(None, 1)]                                                  |\n","|('a', 'c', 'f')|[(None, 1)]                                                  |\n","|('b', 'c', 'f')|[(None, 1)]                                                  |\n","|('d',)         |[(None, 1), ('a', 1), ('b', 1), ('e', 1)]                    |\n","|('e',)         |[(None, 1), ('a', 1), ('b', 1), ('d', 1)]                    |\n","|('a', 'd')     |[(None, 1), ('b', 1), ('e', 1)]                              |\n","|('a', 'e')     |[(None, 1), ('b', 1), ('d', 1)]                              |\n","|('b', 'd')     |[(None, 1), ('a', 1), ('e', 1)]                              |\n","|('b', 'e')     |[(None, 1), ('a', 1), ('d', 1)]                              |\n","+---------------+-------------------------------------------------------------+\n","only showing top 20 rows\n","\n"]}],"source":["def format_value_to_list(pattern):\n","  #On transforme les uniques tuples liés à une clé en liste\n","  key, value = pattern\n","  return (key,[value])\n","\n","def reduce_subpatterns(elem1,elem2):\n","  #On concatène les listes de tuples\n","  return elem1 + elem2\n","\n","#Formattage en liste\n","combined_rules = subpatterns_rdd.map(format_value_to_list)\n","combined_rules = combined_rules.reduceByKey(reduce_subpatterns)\n","# Output as dataframe\n","combined_rules.map(format_tuples).toDF(['subpatterns', 'combined_rules']).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"Uh69x3a8Ma0P"},"source":["### 2.5. Map to Association Rules (15 points)\n","\n","Enfin, la dernière étape de l'algorithme consiste à créer les règles d'association pour effectuer la MBA. Le but de cette fonction Map est de calculer le niveau **de confiance** de l'achat d'un produit, sachant qu'il y a déjà un ensemble de produits dans le panier. Ainsi, la KEY du sous-motif est l'ensemble des produits placés dans le panier et, pour chaque produit présent dans la liste des règles, c'est-à-dire dans la VALEUR, la confiance peut être calculée comme :\n","\n","\\begin{align*}\n","\\frac{\\text{nombre de fois où le produit a été acheté avec KEY}}{\\text{nombre de fois où la KEY est apparue}}\n","\\end{align*}\n","\n","Pour l'exemple donné dans la figure \"workflow\", *le café* a été acheté 20 fois et, dans 17 d'entre eux, le *lait* a été acheté ensemble. Ensuite, le niveau de confiance pour acheter du *lait* sachant que *le café* est dans le panier est $\\frac{17}{20}=0,85$, ce qui signifie que dans 85% des cas où le café a été acheté, le lait a aussi été acheté.\n","\n","Implémentez la fonction **map_to_assoc_rules** qui calcule le niveau de confiance pour chaque sous-motif.\n","\n","Pour le toy dataset, la sortie attendue est:\n","___\n","Finally, the last step of the algorithm is to create the association rules to perform the market basket analysis. The goal of this map function is to calculate the **confidence** level of buying a product, knowing that there is already a set of products in the basket. Thus, the KEY of the subpattern is the set of products placed in the basket and, for each product present in the list of rules, i.e., in the VALUE, the confidence can be calculated as:\n","\n","\\begin{align*}\n","\\frac{\\text{number of times the product was bought together with KEY }}{\\text{number of times the KEY appeared}}\n","\\end{align*}\n","\n","For the example given in the Figure workflow, *coffee* was bought 20 times and, in 17 of them, *milk* was bought together. Then, the confidence level of buying *milk* knowing that *coffee* is in the basket is $\\frac{17}{20} = 0.85$, which means that in 85% of the times the coffee was bought, milk was purchased as well.\n","\n","Implement the **map_to_assoc_rules** function that calculates the confidence level for each subpattern.\n","\n","For the toy dataset, the expected output is:\n","<pre style=\"align:center; border:1px solid black;font-size: 8pt; line-height: 1.1; height: auto; width: 57em; padding-left:5px\">\n","<code>\n","+---------------+------------------------------------------------------------------+\n","|patterns       |association_rules                                                 |\n","+---------------+------------------------------------------------------------------+\n","|('a',)         |[('b', 1.0), ('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]      |\n","|('a', 'b')     |[('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]                  |\n","|('b',)         |[('a', 0.5), ('c', 0.75), ('f', 0.25), ('d', 0.25), ('e', 0.25)]  |\n","|('a', 'b', 'c')|[]                                                                |\n","|('b', 'c')     |[('a', 0.3333333333333333), ('f', 0.3333333333333333)]            |\n","|('a', 'c')     |[('b', 1.0), ('f', 1.0)]                                          |\n","|('a', 'b', 'f')|[]                                                                |\n","|('b', 'f')     |[('a', 1.0), ('c', 1.0)]                                          |\n","|('a', 'f')     |[('b', 1.0), ('c', 1.0)]                                          |\n","|('c',)         |[('a', 0.3333333333333333), ('b', 1.0), ('f', 0.3333333333333333)]|\n","|('a', 'c', 'f')|[]                                                                |\n","|('c', 'f')     |[('a', 1.0), ('b', 1.0)]                                          |\n","|('f',)         |[('a', 1.0), ('b', 1.0), ('c', 1.0)]                              |\n","|('b', 'c', 'f')|[]                                                                |\n","|('a', 'b', 'd')|[]                                                                |\n","|('b', 'd')     |[('a', 1.0), ('e', 1.0)]                                          |\n","|('a', 'd')     |[('b', 1.0), ('e', 1.0)]                                          |\n","|('a', 'b', 'e')|[]                                                                |\n","|('b', 'e')     |[('a', 1.0), ('d', 1.0)]                                          |\n","|('a', 'e')     |[('b', 1.0), ('d', 1.0)]                                          |\n","+---------------+------------------------------------------------------------------+\n","</code>\n","</pre>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":337,"status":"ok","timestamp":1664683820557,"user":{"displayName":"Yoann Monteiro","userId":"02171426768575630386"},"user_tz":240},"id":"DPrbn5CfMa0P","outputId":"a3df06c2-9f70-491d-b1eb-17065e19d74f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------+------------------------------------------------------------------+\n","|patterns       |association_rules                                                 |\n","+---------------+------------------------------------------------------------------+\n","|('a',)         |[('b', 1.0), ('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]      |\n","|('b',)         |[('a', 0.5), ('c', 0.75), ('f', 0.25), ('d', 0.25), ('e', 0.25)]  |\n","|('c',)         |[('a', 0.3333333333333333), ('b', 1.0), ('f', 0.3333333333333333)]|\n","|('f',)         |[('a', 1.0), ('b', 1.0), ('c', 1.0)]                              |\n","|('a', 'b')     |[('c', 0.5), ('f', 0.5), ('d', 0.5), ('e', 0.5)]                  |\n","|('a', 'c')     |[('b', 1.0), ('f', 1.0)]                                          |\n","|('a', 'f')     |[('b', 1.0), ('c', 1.0)]                                          |\n","|('b', 'c')     |[('a', 0.3333333333333333), ('f', 0.3333333333333333)]            |\n","|('b', 'f')     |[('a', 1.0), ('c', 1.0)]                                          |\n","|('c', 'f')     |[('a', 1.0), ('b', 1.0)]                                          |\n","|('a', 'b', 'c')|[]                                                                |\n","|('a', 'b', 'f')|[]                                                                |\n","|('a', 'c', 'f')|[]                                                                |\n","|('b', 'c', 'f')|[]                                                                |\n","|('d',)         |[('a', 1.0), ('b', 1.0), ('e', 1.0)]                              |\n","|('e',)         |[('a', 1.0), ('b', 1.0), ('d', 1.0)]                              |\n","|('a', 'd')     |[('b', 1.0), ('e', 1.0)]                                          |\n","|('a', 'e')     |[('b', 1.0), ('d', 1.0)]                                          |\n","|('b', 'd')     |[('a', 1.0), ('e', 1.0)]                                          |\n","|('b', 'e')     |[('a', 1.0), ('d', 1.0)]                                          |\n","|('d', 'e')     |[('a', 1.0), ('b', 1.0)]                                          |\n","|('a', 'b', 'd')|[]                                                                |\n","|('a', 'b', 'e')|[]                                                                |\n","|('a', 'd', 'e')|[]                                                                |\n","|('b', 'd', 'e')|[]                                                                |\n","+---------------+------------------------------------------------------------------+\n","\n"]}],"source":["from copy import deepcopy\n","def map_to_assoc_rules(rule):\n","    key, value = rule[0],deepcopy(rule[1])\n","\n","    #correspond au tuple value[i] = (None, x) qu'on mémorise et qu'on retire de la liste\n","    i = 0\n","    while value[i][0] != None:\n","      i +=1\n","    _, key_count = value[i]\n","    value.remove(value[i])\n","\n","    #On crée une nouvelle liste de tuples avec les probabilités\n","    value_list = []\n","    for r in value:\n","      value_list.append((r[0],r[1]/key_count))\n","    yield(key, value_list)\n","\n","assoc_rules = combined_rules.flatMap(map_to_assoc_rules)\n","\n","# Output as dataframe\n","assoc_rules.map(format_tuples).toDF(['patterns', 'association_rules']).show(26,truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"BPV5g2hwMa0U"},"source":["## 3. Instacart dataset (35 points)\n","\n","Avec votre algorithme MBA prêt à être utilisé, il est maintenant temps de travailler sur l'ensemble de données réel. Pour cette partie du TP, téléchargez le dataset [instacart](https://drive.google.com/file/d/1pXjqPz1RbL40yCGWnTCbmW_ZXrjlfJi4/view?usp=sharing) et lisez sa [description](https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b) pour comprendre la structure de l'ensemble de données.\n","\n","Avant d'appliquer l'algorithme développé sur l'ensemble de données instacart, vous devez d'abord filtrer les transactions pour qu'elles soient au même format défini par votre algorithme (une transaction par ligne). Pour manipuler les données, nous pouvons utiliser le bloc de données de Spark et le module SQL présenté dans la section 1.\n","\n","La cellule de code suivante utilise le module Spark SQL pour lire les commandes de ``order_products__train.csv`` et les informations détaillées de ``orders.csv`` et ``products.csv`` pour construire une dataframe qui contient un liste de tous les produits jamais achetés par chaque utilisateur.\n","___\n","With your MBA algorithm ready to be used, now it is time to work on the real dataset. For this part of the TP, download the [instacart](https://drive.google.com/file/d/1pXjqPz1RbL40yCGWnTCbmW_ZXrjlfJi4/view?usp=sharing) dataset and read its [description](https://gist.github.com/jeremystan/c3b39d947d9b88b3ccff3147dbcf6c6b) to understand how the dataset is structured. \n","\n","Before applying the developed algorithm on the instacart dataset, you must first filter the transactions to be in the same format defined by your algorithm (one transaction per row). To manipulate the data, we can use Spark's data frame and the SQL module presented in Section 1.\n","\n","The following code cell uses the Spark SQL module to read the orders from the ``order_products__train.csv`` and the detailed information from ``orders.csv`` and ``products.csv`` to construct a data frame that contains a list of all products ever purchased by each user."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6oB1eTkeMa0W","outputId":"052fd204-440e-4e8c-fe92-7b86966132e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["order_products__train.csv\n","+--------+----------+-----------------+---------+\n","|order_id|product_id|add_to_cart_order|reordered|\n","+--------+----------+-----------------+---------+\n","|       1|     49302|                1|        1|\n","|       1|     11109|                2|        1|\n","|       1|     10246|                3|        0|\n","|       1|     49683|                4|        0|\n","|       1|     43633|                5|        1|\n","+--------+----------+-----------------+---------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["orders.csv\n","+--------+-------+--------+------------+---------+-----------------+----------------------+\n","|order_id|user_id|eval_set|order_number|order_dow|order_hour_of_day|days_since_prior_order|\n","+--------+-------+--------+------------+---------+-----------------+----------------------+\n","| 2539329|      1|   prior|           1|        2|                8|                  null|\n","| 2398795|      1|   prior|           2|        3|                7|                  15.0|\n","|  473747|      1|   prior|           3|        3|               12|                  21.0|\n","| 2254736|      1|   prior|           4|        4|                7|                  29.0|\n","|  431534|      1|   prior|           5|        4|               15|                  28.0|\n","+--------+-------+--------+------------+---------+-----------------+----------------------+\n","only showing top 5 rows\n","\n","products.csv\n","+----------+--------------------+--------+-------------+\n","|product_id|        product_name|aisle_id|department_id|\n","+----------+--------------------+--------+-------------+\n","|         1|Chocolate Sandwic...|      61|           19|\n","|         2|    All-Seasons Salt|     104|           13|\n","|         3|Robust Golden Uns...|      94|            7|\n","|         4|Smart Ones Classi...|      38|            1|\n","|         5|Green Chile Anyti...|       5|           13|\n","+----------+--------------------+--------+-------------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 18:==========================>                              (8 + 9) / 17]\r"]},{"name":"stdout","output_type":"stream","text":["+-------+--------------------------------------------------------------------------------+\n","|user_id|                                                                        products|\n","+-------+--------------------------------------------------------------------------------+\n","|      1|[Soda, Organic String Cheese, 0% Greek Strained Yogurt, XL Pick-A-Size Paper ...|\n","|      2|[Organic Roasted Turkey Breast, Gluten Free Whole Grain Bread, Plantain Chips...|\n","|      5|[Organic Raw Agave Nectar, Organic Large Extra Fancy Fuji Apple, Sharp Chedda...|\n","|      7|[Panama Peach Antioxidant Infusion, Antioxidant Infusions Beverage Malawi Man...|\n","|      8|[Shallot, Organic SprouTofu Silken Tofu, Nutritional Yeast Seasoning, Organic...|\n","+-------+--------------------------------------------------------------------------------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df_order_prod = spark.read.csv('instacart/order_products__train.csv', header=True, sep=',', inferSchema=True)\n","print('order_products__train.csv')\n","df_order_prod.show(5)\n","\n","df_orders = spark.read.csv('instacart/orders.csv', header=True, sep=',', inferSchema=True)\n","print('orders.csv')\n","df_orders.show(5)\n","\n","df_products = spark.read.csv('instacart/products.csv', header=True, sep=',', inferSchema=True)\n","print('products.csv')\n","df_products.show(5)\n","\n","\n","\"\"\"\n","List of products ever purchased by each user\n","\"\"\"\n","# USING SQL\n","df_order_prod.createOrReplaceTempView(\"order_prod\") # creates table 'order_prod'\n","df_orders.createOrReplaceTempView(\"orders\") # creates table 'orders'\n","df_products.createOrReplaceTempView(\"products\") # creates table 'products'\n","spark.sql('SELECT o.user_id, COLLECT_LIST(p.product_name) AS products' \n","               ' FROM orders o '\n","               ' INNER JOIN order_prod op ON op.order_id = o.order_id'\n","               ' INNER JOIN products p    ON op.product_id = p.product_id'\n","               ' GROUP BY user_id ORDER BY o.user_id').show(5, truncate=80)\n","\n","\n","# USING DATAFRAME OPERATIONS\n","# df_orders.join(df_order_prod, df_order_prod.order_id == df_orders.order_id, 'inner')\\\n","# .join(df_products, df_products.product_id == df_order_prod.product_id, 'inner')\\\n","# .groupBy(df_orders.user_id).agg(f.collect_list(df_products.product_name).alias('products'))\\\n","# .orderBy(df_orders.user_id).show(5, truncate=80)"]},{"cell_type":"markdown","metadata":{"id":"JEqVeqhkMa0a"},"source":["### 3.1 Perspectives commerciales (20 points) \n","\n","Maintenant, vous êtes le *data scientist*. En ne considérant que les commandes de ``order_products__train.csv``, l'utilisation du module Spark SQL, performant avec SQL ou dataframe, pour répondre aux questions suivantes:\n","\n","1. Quels sont les 10 produits les plus susceptibles d'être commandé de nouveau? Ne considérez que les produits achetés au moins 40 fois pour cette tâche.\n","2. Quels sont les 3 produits les plus achetés dans chaque département?\n","4. Quelle est la taille moyenne du panier pour chaque jour de la semaine?\n","    - utilisez un barplot pour visualiser vos résultats\n","\n","**La sortie de ces questions doit contenir le NOM des produits, pas leur ID.**\n","___\n","Now, you are the data scientist. Considering only the orders of ``order_products__train.csv``, use of Spark SQL module, performing with SQL or data frame, to answer the following questions:\n","\n","1. What are the top 10 products which have the highest probability of being reordered? Consider only products purchased at least 40 times for this task.\n","2. What are the top 3 most purchased products of each department?\n","4. What is the average basket size for each day of the week?\n","- Hint: use a barplot to visualize your results\n","\n","**The output of those questions must contain the products' NAME, not their ID.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-IxS71fcN_Z0","outputId":"90e3e148-9cd9-43d0-cc0f-dcb7c8b13a62"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 71:=========>                                                (1 + 5) / 6]\r"]},{"name":"stdout","output_type":"stream","text":["+-------------------------------------+--------------+------------+-------------------+\n","|                         product_name|count_reorders|total_orders|reorder_probability|\n","+-------------------------------------+--------------+------------+-------------------+\n","|                 2% Lactose Free Milk|            86|          92|  93.47826086956522|\n","|                 Organic Low Fat Milk|           336|         368|  91.30434782608695|\n","|            100% Florida Orange Juice|            53|          59|  89.83050847457628|\n","|Original Sparkling Seltzer Water Cans|            40|          45|  88.88888888888889|\n","|              Organic Spelt Tortillas|            72|          81|  88.88888888888889|\n","|                               Banana|         16557|       18726|  88.41717398269785|\n","|                   Petit Suisse Fruit|           106|         120|  88.33333333333333|\n","|               Organic Lowfat 1% Milk|           426|         483|  88.19875776397515|\n","|  Organic Lactose Free 1% Lowfat Milk|           237|         269|  88.10408921933085|\n","|                       1% Lowfat Milk|           405|         461|  87.85249457700651|\n","+-------------------------------------+--------------+------------+-------------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#1. Quels sont les 10 produits les plus susceptibles d'être commandé de nouveau? Ne considérez que les produits achetés au moins 40 fois pour cette tâche.\n","\n","spark.sql('SELECT product_name, count_reorders, total_orders, count_reorders/total_orders*100 AS reorder_probability FROM'\n","                  '('\n","                        'SELECT p.product_name, SUM(reordered) AS count_reorders, COUNT(*) AS total_orders' \n","                        ' FROM order_prod op'\n","                        ' JOIN products p ON p.product_id = op.product_id'\n","                        ' GROUP BY p.product_name'\n","                        ' HAVING total_orders >= 40'\n","                  ')'\n","           ' ORDER BY reorder_probability DESC'\n","           ' LIMIT 10'\n","          ).show(truncate=100)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6Seg7coOZPs","outputId":"3dce3937-1371-4cda-ce32-2f96607486ad"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 76:>                                                         (0 + 6) / 6]\r"]},{"name":"stdout","output_type":"stream","text":["+-----------------------------------------------------+-------------+---------------+----+\n","|                                         product_name|department_id|count_commandes|rank|\n","+-----------------------------------------------------+-------------+---------------+----+\n","|                                          Blueberries|            1|           2323|   1|\n","|                             Organic Broccoli Florets|            1|           1361|   2|\n","|                           Organic Whole Strawberries|            1|           1213|   3|\n","|                                Roasted Almond Butter|            2|            174|   1|\n","|              Light CocoWhip! Coconut Whipped Topping|            2|             86|   2|\n","|                             Roasted Unsalted Almonds|            2|             62|   3|\n","|                               100% Whole Wheat Bread|            3|           2298|   1|\n","|                   Organic Bread with 21 Whole Grains|            3|            938|   2|\n","|                                      Sourdough Bread|            3|            738|   3|\n","|                                               Banana|            4|          18726|   1|\n","|                               Bag of Organic Bananas|            4|          15480|   2|\n","|                                 Organic Strawberries|            4|          10894|   3|\n","|                                      Sauvignon Blanc|            5|            295|   1|\n","|                                   Cabernet Sauvignon|            5|            237|   2|\n","|                                                 Beer|            5|            224|   3|\n","|                                       Taco Seasoning|            6|            405|   1|\n","|              Organic Sea Salt Roasted Seaweed Snacks|            6|            345|   2|\n","|            New Mexico Taco Skillet Sauce For Chicken|            6|            224|   3|\n","|                           Sparkling Water Grapefruit|            7|           3359|   1|\n","|                                         Spring Water|            7|           2225|   2|\n","|                                 Lime Sparkling Water|            7|           1966|   3|\n","|Double Duty Advanced Odor Control Clumping Cat Litter|            8|             84|   1|\n","|                          24/7 Performance Cat Litter|            8|             76|   2|\n","|                            Instant Action Cat Litter|            8|             73|   3|\n","|                     Organic Tomato Basil Pasta Sauce|            9|            772|   1|\n","|                                       Marinara Sauce|            9|            754|   2|\n","|                                          Basil Pesto|            9|            699|   3|\n","|                                          Dried Mango|           10|            446|   1|\n","|                                  Organic Rolled Oats|           10|            259|   2|\n","|                           Organic Black Mission Figs|           10|            125|   3|\n","|                                   Lavender Hand Soap|           11|            258|   1|\n","|                                         Cotton Swabs|           11|            258|   1|\n","|                              Lemon Verbena Hand Soap|           11|            191|   3|\n","|                    Boneless Skinless Chicken Breasts|           12|           2088|   1|\n","|                                 Ground Turkey Breast|           12|            958|   2|\n","|                     Boneless Skinless Chicken Breast|           12|            943|   3|\n","|                               Extra Virgin Olive Oil|           13|           2068|   1|\n","|                                 Creamy Peanut Butter|           13|            991|   2|\n","|                                 Creamy Almond Butter|           13|            850|   3|\n","|                                   Honey Nut Cheerios|           14|           1218|   1|\n","|                    Organic Old Fashioned Rolled Oats|           14|            747|   2|\n","|                                   Raisin Bran Cereal|           14|            600|   3|\n","|                                  Organic Black Beans|           15|           1576|   1|\n","|                            No Salt Added Black Beans|           15|           1250|   2|\n","|                               Organic Garbanzo Beans|           15|           1141|   3|\n","|                                   Organic Whole Milk|           16|           4908|   1|\n","|                                  Organic Half & Half|           16|           2646|   2|\n","|                                          Half & Half|           16|           2424|   3|\n","|                           100% Recycled Paper Towels|           17|           1183|   1|\n","|                         Sustainably Soft Bath Tissue|           17|            821|   2|\n","|                                        Aluminum Foil|           17|            407|   3|\n","|     Baby Food Stage 2 Blueberry Pear & Purple Carrot|           18|            310|   1|\n","|                Spinach Peas & Pear Stage 2 Baby Food|           18|            268|   2|\n","|                Gluten Free SpongeBob Spinach Littles|           18|            259|   3|\n","|                 Lightly Salted Baked Snap Pea Crisps|           19|            991|   1|\n","|  Pretzel Crisps Original Deli Style Pretzel Crackers|           19|            753|   2|\n","|                                  Sea Salt Pita Chips|           19|            707|   3|\n","|                                      Original Hummus|           20|           2858|   1|\n","|                                 Uncured Genoa Salami|           20|           1788|   2|\n","|                              Organic Extra Firm Tofu|           20|           1186|   3|\n","|                            Organic Riced Cauliflower|           21|            823|   1|\n","|                          Peanut Butter Ice Cream Cup|           21|            261|   2|\n","|                                 Organic Celery Bunch|           21|            200|   3|\n","+-----------------------------------------------------+-------------+---------------+----+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#2. Quels sont les 3 produits les plus achetés dans chaque département?\n","\n","spark.sql(\n","    #On ne sélectionne que les 3 meilleurs rangs et on ordonne par (département, rang)\n","    'SELECT * FROM'\n","        '('\n","          #On affecte des rangs aux produits selon le nombre de commandes et le département\n","          'SELECT product_name, department_id, count_commandes, RANK() OVER (PARTITION BY department_id ORDER BY count_commandes DESC) AS rank FROM '   \n","             '('\n","                  #On sélectionne les noms de produits, leur département et le nombre de commandes selon le département\n","                  ' SELECT p.product_name, p.department_id, COUNT(*) AS count_commandes'\n","                  ' FROM order_prod op'\n","                  ' JOIN products p ON p.product_id = op.product_id'\n","                  ' GROUP BY p.product_name, p.department_id'\n","              ')'\n","        ')'\n","        ' WHERE rank <=3'\n","\n","        #Ici, department_id est une chaîne de caractères,\n","        #Pour obtenir un classement correct des 21 départements, on le convertit en entier avec CAST()\n","        ' ORDER BY CAST(department_id AS INTEGER), rank'\n","        ).show(63, truncate=80)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MODKAihOdCS","outputId":"e4d76b8f-8a7a-43c8-f66c-8f3979022633"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 87:>                                                       (0 + 16) / 17]\r"]},{"name":"stdout","output_type":"stream","text":["+---------+-------------------+\n","|order_dow|average_basket_size|\n","+---------+-------------------+\n","|        0| 11.797778991443655|\n","|        1| 10.470618137454249|\n","|        2|   9.96103976673491|\n","|        3|   9.84133358832154|\n","|        4|  9.742527727301209|\n","|        5| 10.163736642537057|\n","|        6| 10.966562615734617|\n","+---------+-------------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["#3. Quelle est la taille moyenne du panier pour chaque jour de la semaine?\n","\n","spark.sql(\n","    #On calcule la moyenne selon le jour de la semaine \n","    'SELECT order_dow, AVG(basket_size) AS average_basket_size FROM '\n","      '('\n","          #On sélectionne le numéro de commande, la taille du panier de commande et le jour de la semaine correspondant \n","          'SELECT order_dow, o.order_id, COUNT(product_name) AS basket_size'\n","          ' FROM order_prod op'\n","          ' JOIN orders o ON o.order_id = op.order_id'\n","          ' JOIN products p ON p.product_id = op.product_id'\n","          ' GROUP BY order_dow, o.order_id'\n","      ')'\n","      ' GROUP BY order_dow'\n","      ' ORDER BY order_dow'\n",").show(7, truncate=80)\n"]},{"cell_type":"markdown","metadata":{"id":"PEWqTH1QMa0a"},"source":["### 3.2 MBA pour le training set / Run MBA for the training set (15 points)\n","\n","En utilisant les commandes du ``order_products__train.csv``, créez un bloc de données où chaque ligne contient la colonne ``transaction`` avec la liste des produits achetés, de manière similaire à le toy dataset. Ensuite, exécutez l'algorithme MBA pour cet ensemble de transactions.\n","\n","- Vous devez signaler le temps passé pour effectuer cette tâche.\n","- La sortie doit contenir le nom des produits.\n","___\n","Using the orders from the ``order_products__train.csv``, create a data frame where each row contains the column “transaction” with the list of purchased products, similarly to the toy dataset. In sequence, run the MBA algorithm for this set of transactions. \n","\n","- You must report the time spent to perform this task.\n","- Output must contain the products' name."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vxZh_f3hMa0b","outputId":"5eccfd54-deb4-444a-d3ab-c893d355cd7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|order_id|                                                                                                                                           transaction|\n","+--------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|    1139|Banana;Organic Strawberries;Red Vine Tomato;Organic Bakery Hamburger Buns Wheat - 8 CT;Guacamole;Picnic Potato Salad;Cinnamon Rolls with Icing;Flak...|\n","|    1143|Natural Premium Coconut Water;Calming Lavender Body Wash;Unscented Long Lasting Stick Deodorant;Organic Raspberries;Organic Blueberries;Organic Has...|\n","|    1342|Bag of Organic Bananas;Seedless Cucumbers;Organic Mandarins;Organic Strawberries;Versatile Stain Remover;Pink Lady Apples;Chicken Apple Sausage;Raw...|\n","|    1468|Banana;Organic Ginger Root;Cage Free Grade AA Large White Eggs;Active Dry Yeast;Bartlett Pears;Natural Mini Pork Pepperoni;Organic Lacinato (Dinosa...|\n","|    1591|Cracked Wheat;Organic Greek Whole Milk Blended Vanilla Bean Yogurt;Navel Oranges;Spinach;Original Patties (100965) 12 Oz Breakfast;Cinnamon Multigr...|\n","|    1721|                Organic Reduced Fat Milk;Organic Whole Milk;Organic Rolled Oats;Organic Thompson Seedless Raisins;Whole Milk Plain Yogurt;Tomato Paste|\n","|    2711|                                                                       Granny Smith Apples;Honeycrisp Apples;Alpine Spring Water;Mountain Spring Water|\n","|    2888|Creamy Almond Butter;100% Pure Vegetable Oil;Organic Grade A Large Brown Eggs;Ground Cumin;Organic Raspberries;Organic Strawberries;Organic Large E...|\n","|    3179|Lime Sparkling Water;Caramel Almond and Sea Salt Nut Bar;Krinkle Cut Classic Barbecue Potato Chips;Sparkling Water Grapefruit;Honey Bunny Grahams;W...|\n","|    4092|Limes;Mozzarella Cheese;Kale Greens;Large Lemon;Carrots;Fresh Ginger Root;Salted Butter;Organic Extra Firm Tofu;Organic Baby Spinach;Garlic;Organic...|\n","|    4383|                                   Organic Orange Soda;Organic Professor Fizz Zero Calorie Soda;Tzatziki;Cauliettes Culinary Cuts;Organic Strawberries|\n","|    4519|                                                                                    Beet Apple Carrot Lemon Ginger Organic Cold Pressed Juice Beverage|\n","|    4935|                                                                                                                                                 Vodka|\n","|    5117|Organic Strawberries;Sparkling Water Grapefruit;Organic Broccoli Crowns;Organic Rainbow Carrots;Organic Zucchini;Organic Fat Free Milk;Organic Dark...|\n","|    5433|Organic Banana;100% Recycled Paper Towels;Dark Chocolate Chips;Organic Old Fashioned Rolled Oats;Organic Bunny Fruit Snacks Berry Patch;Organic Bun...|\n","|    6357|Fresh Mozzarella Ball;Grated Parmesan;Organic Basil;Provolone;Gala Apples;Panko Bread Crumbs;Italian Pasta Sauce Basilico Tomato, Basil & Garlic;Gl...|\n","|    6552|Sparkling Water Grapefruit;Gluten Free Fettuccine;Plain Almond Milk Yogurt;Honey & Maple Turkey Breast;Trop50 No Pulp Orange Juice;Just Green Unswe...|\n","|    6773|                                                                                  Unrefined Virgin Coconut Oil;100% Recycled Paper Towels;Cheese Pizza|\n","|    7168|Natural Calm Plus Calcium;Pure Kosher Dills;Organic Avocado;Dry Rubbed Thick Sliced Pork Bacon;Everything Deli Style Pretzel Crisps Crackers;Organi...|\n","|    7865|                                                                                                           Taboule Salad;Organic Roasted Garlic Hummus|\n","|    8231|Dill Pickle Spears;Granny Smith Apples;Greek Toasted Coconut Vanilla Non Fat Yogurt;Light & Fit Strawberry Cheesecake Greek Nonfat Yogurt;Less Sodi...|\n","|    8932|Sea Salt & Pepper Popcorn;smartwater® Electrolyte Enhanced Water;Ultra Soft Facial Tissues;Black Cherry Lowfat Yogurt;Granny Smith Apples;100% Lact...|\n","|    9206|Organic Cameo Apple;Banana;Organic Raw Kombucha Gingerade;Orange Peel In Dark Chocolate Bar;Organic Heavy Whipping Cream;Organic Large Grade AA Bro...|\n","|    9517|Ultra Downy® Mountain Spring™ Liquid Fabric Conditioner 51 Fl oz. 60 loads Fabric Enhancers;Magic Eraser Extra Power;Mega Shower Foamer with Ultra ...|\n","|    9879|Vanilla Unsweetened Almond Milk;Banana;100% Recycled Paper Towels;Simply White Natural Clean Mint Fluoride Toothpaste;Free & Clear Natural Dish Liq...|\n","|   10162|Banana;Cantaloupe;Yellow Grape Tomatoes;Organic Strawberries;Organic Turkey Bacon;Organic Radicchio Castelfranco;Organic Romaine;smartwater® Electr...|\n","|   10362|Crinkle Cut French Fries;The Ultimate Beefless Burger;Original Tofurky Deli Slices;Slow Roasted Lightly Seasoned Chick'n;Organic Gala Apples;Hass A...|\n","|   10371|Organic Garam Masala;Organic Ginger Root;Organic Green Cardamom;Organic Curry Powder;Organic Red Lentils;Organic Yellow Onion;Mango Chunks;Organic ...|\n","|   11478|                             Light Brown Sugar;Unsalted Pure Irish Butter;Gluteen Free Center Cut Applewood Smoked Uncured Bacon;Michigan Organic Kale|\n","|   12182|                                                                                    Whole Wheat Flour;Organic Banana;Organic Old Fashioned Rolled Oats|\n","+--------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n","only showing top 30 rows\n","\n","CPU times: user 1.72 ms, sys: 1.57 ms, total: 3.28 ms\n","Wall time: 869 ms\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["%%time\n","\"\"\"\n","TODO: create a query to create and sctruct the transactions\n","\"\"\"\n","\n","training_set_df = spark.sql('SELECT order_id, CONCAT_WS(\";\",COLLECT_LIST(p.product_name)) AS transaction'\n","          ' FROM order_prod op'\n","          ' JOIN products p ON p.product_id = op.product_id'\n","          ' GROUP BY order_id'\n",")\n","\n","training_set_df.show(30,truncate=150)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dewN0YUEMa0h","outputId":"8f6de9bd-740e-4b88-a680-686479f1eee8"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 5.56 ms, sys: 0 ns, total: 5.56 ms\n","Wall time: 766 ms\n"]}],"source":["%%time\n","\"\"\"\n","TODO: run the MBA algorithm and show the first 5 association rules\n","\"\"\"\n","#1er map + reduce\n","training_set_rdd = training_set_df.rdd\n","training_patterns = training_set_rdd.flatMap(map_to_patterns)\n","combined_training_patterns = training_patterns.reduceByKey(reduce_patterns)\n","#combined_training_patterns.map(format_tuples).toDF(['patterns', 'combined_occurences']).show(150, truncate=False)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDq0zlOznilW","outputId":"24be3681-ad1a-47c5-912b-19e5a0f9d73a"},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 105:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|                                                                                                               subpatterns|                                                                                                                                        combined_rules|\n","+--------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|('Organic Fire Roasted Diced Tomatoes', 'Superfoods Organic Pears, Blueberries & Beets + Super Chia Fruit & Veggie Blend')|[('Cantaloupe', 1), (None, 1), ('Grated Parmesan', 1), ('Asparagus', 1), ('Organic Hass Avocado', 1), ('Organic Free Range Low Sodium Chicken Broth...|\n","|                                                                                               ('Maple Glazed Honey Ham',)|[('Dark Italian Roast Ground Coffee', 1), ('Organic Dark Chocolate Peanut Butter Cups', 1), ('Lunchables Cracker Stackers Ham & American Cheese', 1...|\n","|                                                                                        ('Club Soda', 'Fresh Goat Cheese')|[('Almond Paste', 1), (None, 1), ('Salame, Italian Dry', 1), ('Caesar Salad', 1), ('Fresh European Style Baby Spinach', 1), ('Heavy Whipping Cream'...|\n","|                                                                                  ('Blueberries', 'Organic Baby Broccoli')|[('Organic Chunky Vegetable Soup', 1), ('Lime Slim Can', 1), ('Roasted Garlic Alfredo Pasta Sauce', 1), ('Fancy Eggplant', 1), ('Fresh Cauliflower'...|\n","|                                                        ('Natural Shredded Mexican Style Four Cheese', 'Organic Cucumber')|[('Asparagus', 1), ('Red Peppers', 1), (None, 8), ('Seedless Grapes Green', 1), ('Super Colossal Pimento Stuffed Spanish Queen Olives', 1), ('Hazel...|\n","|                                                                           ('Premium Pinto Beans', 'Tiny Twists Pretzels')|[('Oreo Chocolate Sandwich Cookies', 1), ('XL Emerald White Seedless Grapes', 1), ('Strawberry Candy Twists', 1), (None, 1), ('Seedless Red Grapes'...|\n","|                                                                  ('Milk, Vitamin D', 'Porcini & Truffle Mezzelune Pasta')|[('Kava Stress Relief Tea', 1), (None, 1), ('Bag of Organic Bananas', 1), ('Organic Frozen Mango Chunks', 1), ('Okra', 1), ('Organic Acai Berry Smo...|\n","|                                                                (\"Crunchy Oats 'n Honey Granola\", 'Wild Mushroom Ravioli')|[('Blueberries', 1), (None, 1), ('Marble Jack Cheese', 1), ('Shredded Carrots', 1), ('Crispy Chicken Strips 100% All Natural Fully Cooked', 1), ('S...|\n","|                                                              ('Gluten Free Pretzel Twists', 'Haunted Ghost Pepper Chips')|[('Super Dark Coconut Ash & Banana Chocolate Bar', 1), (None, 1), ('Kettle Chips Vegan Cheddar', 1), ('Gluten Free Dream Cookies Chocolate Vanilla ...|\n","|                                                                    ('Naturals Chicken Nuggets', 'Whole Grain Pita Bread')|[('Organic Navel Orange', 1), ('Gelatin Free Snacks Natural Orange Gels', 1), ('Soft Pretzel Burger Buns', 1), (None, 2), ('Organic Free Range Chic...|\n","|                                               ('Emmentaler Swiss Slices', 'Organic Stage 2 Chicken Brown Rice Baby Food')|[('Jalapeno Peppers', 1), (None, 1), ('Organic Stage 2 Apple Oatmeal With Cinnamon Baby Food', 1), ('Organic Hot House Tomato', 1), ('Apple Peach O...|\n","|                                                                                 ('1 Ply Paper Towels', 'Go Pack Tissues')|[('Disinfecting Wipes, Lemon', 1), ('Orange Bell Pepper', 1), ('Natural Tomato Paste', 1), ('Half Dozen Grade A White Eggs Large', 1), (None, 1), (...|\n","|                                                          ('Microwave Ready Original Bacon', 'Organic Granny Smith Apple')|[('Unsweetened Vanilla Almond Milk', 1), (None, 1), ('Baked Rice and Corn Puffs, Aged White Cheddar', 1), ('Natural Sharp Cheddar Sliced Cheese', 1...|\n","|                                                      ('Peeled Paradise Found Mixed Fruit', 'Tomato & Basil Tomato Sauce')|[('Iced Coffee', 1), ('Solid White Albacore Tuna In Water', 1), ('Polska Kielbasa Sausage', 1), (None, 1), ('Honeycrisp Apples', 1), ('Apple Cider ...|\n","|                                                                             ('Double Rolls Bath Tissue', 'Yellow Onions')|[('Beef Cooking Stock', 1), ('Shaved Brussel Sprouts', 1), ('Large Burrito Flour Tortillas', 1), ('Organic Baby Romaine Lettuce', 2), ('Large Lemon...|\n","|                                        ('All Natural Berry Blast 100% Juice Smoothie', 'Light & Fit Greek Cherry Yogurt')|[('Simple Favorites Five Cheese Rigatoni', 1), (None, 1), ('Mandarin Oranges', 1), ('Strawberry on the Bottom Nonfat Greek Yogurt', 1), ('Marketpla...|\n","|                                                                                       ('Oregano', 'Shredded Red Cabbage')|[('Feta Cheese Crumbles', 1), (None, 1), ('Cage Free Grade A Large Brown Eggs', 1), ('Organic Italian Parsley Bunch', 1), ('Boneless Skinless Chick...|\n","|                                  ('French Vanilla Sugar Free Liquid Coffee Creamer', 'Jet Puffed Miniature Marshmallows')|[('Hummus', 1), (None, 2), ('Sea Salt Made With Organic Grain Rice Chips', 1), ('Condensed Cream of Chicken Soup', 1), ('100% Oatnut Bread', 1), ('...|\n","|                                                         ('Orange Bell Pepper', 'Original Superfood Fruit Smoothie Blend')|[('Organic Dijon Mustard', 1), (None, 1), ('Organic Romaine', 1), ('Icelandic Style Skyr Blueberry Non-fat Yogurt', 1), ('Organic Italian Parsley B...|\n","|                                                                      ('Cherry Soda', 'Vanilla Almond Breeze Almond Milk')|[('Curate Pomme Baya-Apple Berry Sparkling Water', 1), ('Reduced Fat Pepperoni Pizza Frozen Sandwiches', 1), ('Crispy Thin Crust BBQ Recipe Chicken...|\n","|                          ('Goldfish Cheddar Baked Snack Crackers Multi Packs', 'Organic Quinoa & Brown Rice With Garlic')|[('Mixed Fruit Fruit Snacks', 1), ('Asian Chopped Salad with Dressing', 1), ('Gluten Free Blueberry Waffles', 1), ('Sausage, Egg, & Cheese Biscuit'...|\n","|                                                   ('Cold Brew All Black Mocha Coffee', 'Cracker Trio Selected Favorites')|[('Fabric Softener Dryer Sheet Outdoor Fresh 160CT Fabric Enhancers', 1), (None, 1), ('100% Lime Juice', 1), ('Vanilla Coffee Concentrate', 1), ('P...|\n","|                                                                                      ('Homemade Membrillo Quince Paste',)|[('Mango Chunks', 1), ('Chocolates', 1), ('Gluteen Free Center Cut Applewood Smoked Uncured Bacon', 1), (None, 6), ('Organic Mango Chunks', 1), ('O...|\n","|                                                                   ('Club Soda', 'Natural Classic Pork Breakfast Sausage')|[('Unsweetened Almondmilk', 1), ('Organic Italian Parsley Bunch', 1), ('Mild Cheddar Cheese Sticks', 1), ('Organic Greek Nonfat Yogurt With Mixed B...|\n","|                                                                   ('French Baguette', 'Organic Premium Raspberry Spread')|[('Roasted Turkey Breast', 1), (None, 1), ('gelato Coffee Toffee', 1), ('Organic Large Extra Fancy Fuji Apple', 1), ('Banana', 1), ('Organic Whole ...|\n","|                                         ('Mexican Style Four Cheese Shredded Cheese', 'Super Blue Cheese Dressing + Dip')|[('Thin Mint Crisp Oreos', 1), (None, 1), ('Golden Delicious Apple', 1), ('Grade A Large White Eggs', 1), ('Non Fat Black Cherry on the Bottom Gree...|\n","|                                                            ('Lightly Salted Baked Snap Pea Crisps', 'Organic Mayonnaise')|[('Organic Rainbow Chard Vegetable', 1), ('Organic Peeled Whole Baby Carrots', 1), ('Stage 2 Eat Your Colors Organic, Green: Pea, Kiwi, Pear & Avoc...|\n","|                                                                                 ('Apple Juice', 'Ranch Classic Dressing')|[('White Tea and Vitamin E Antibacterial Hand Soap with Moisturizer', 1), ('Cream of Chicken & Mushroom Condensed Soup', 1), (None, 1), ('Casual Na...|\n","|                                          ('Creamy Peanut Butter', 'Ultra Concentrated Original Scent Dishwashing Liquid')|[(None, 4), ('Espresso Ground Coffee', 1), ('Organic YoBaby Vanilla Yogurt', 1), ('Diet 12 Oz Ginger Ale', 1), ('Coke Classic', 1), ('Grapes, Certi...|\n","|                                                        ('Almond Meal/Flour', 'Organic Lemon', 'Ultra Strong Bath Tissue')|                                                                                                                                           [(None, 1)]|\n","|                                         ('Impressions Napkins', 'Milk Chocolate Hot Cocoa Mix', 'Sensitive Toilet Paper')|                                                                                                                                           [(None, 1)]|\n","|                                                                          ('Bare Fruit Banana Chips', 'Raspberry Yoghurt')|[(None, 1), ('Gluten Free Plain Bagels', 1), ('Carrot Ginger', 1), ('Reserve Dark Roast Costa Rica Tarrazú Whole Bean Coffee', 1), ('Corn Tortillas...|\n","|                                            ('Butternut Squash Ravioli', 'Organic Coconut Milk', 'Organic Tomato Cluster')|                                                                                                                                           [(None, 1)]|\n","|                                                                        ('All Natural Virgin Lemonade', 'Banana', 'Limes')|                                                                                                                                           [(None, 4)]|\n","|      ('Caramel Cookie Crunch Gelato', 'Hamburger Helper Classic Ultimate Cheeseburger Macaroni', 'Sparkling Water Berry')|                                                                                                                                           [(None, 1)]|\n","|                                ('Bag of Organic Bananas', 'Strawberries', 'Vanilla Milk Chocolate Almond Ice Cream Bars')|                                                                                                                                           [(None, 1)]|\n","|                                                            ('Olive Hummus', 'Oven Roasted Turkey', 'Sea Salt Pita Chips')|                                                                                                                                           [(None, 1)]|\n","|                                                               ('Blackberries', 'Bread Machine Yeast', 'Seaweed, Roasted')|                                                                                                                                           [(None, 1)]|\n","|                                             ('Classic Hummus', 'Clementines, Bag', 'Natural Sharp Cheddar Sliced Cheese')|                                                                                                                                           [(None, 4)]|\n","|                                   ('I Heart Baby Kale', 'Organic Egg Whites', 'Organic Roasted Red Pepper & Tomato Soup')|                                                                                                                                           [(None, 1)]|\n","|                          ('Organic Garnet Sweet Potato (Yam)', 'Organic Mango', 'Unscented Long Lasting Stick Deodorant')|                                                                                                                                           [(None, 1)]|\n","|                                  ('Organic Red On the Vine Tomato', 'Organic Sliced Peaches', 'Pasta Sauce Tomato Basil')|                                                                                                                                           [(None, 1)]|\n","|                                                        ('Castor Oil', 'Organic Lacinato (Dinosaur) Kale', 'Spring Water')|                                                                                                                                           [(None, 1)]|\n","|                                             ('Orzo Pasta', 'Pepperoni Pizza Frozen Sandwiches', 'Sliced Sourdough Bread')|                                                                                                                                           [(None, 1)]|\n","|                                                     ('Arancita Rossa', 'Organic Butternut Squash', 'Organic Raw Cashews')|                                                                                                                                           [(None, 1)]|\n","|                            ('Organic AppleApple', 'Organic Roasted Sliced Chicken Breast', 'Organic Strawberry Smoothie')|                                                                                                                                           [(None, 2)]|\n","|                                                ('100% Pure Vegetable Oil', 'Pepperoni', 'Sunflower Nuts Roasted/No Salt')|                                                                                                                                           [(None, 1)]|\n","|                                        ('Gluten Free Chocolate Chip Cookies', 'Grated Parmesan', 'Michigan Organic Kale')|                                                                                                                                           [(None, 1)]|\n","|('97% Fat Free Beef Franks', 'Chicken Strips & Fries Tasty American Favorites', 'Hazelnut Fat Free Liquid Coffee Creamer')|                                                                                                                                           [(None, 1)]|\n","|                                  ('Hot Dog Buns', \"Kellogg's Pop-Tarts Frosted Strawberry Pastries\", 'Macaroni & Cheese')|                                                                                                                                           [(None, 1)]|\n","+--------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n","only showing top 50 rows\n","\n","CPU times: user 266 ms, sys: 114 ms, total: 380 ms\n","Wall time: 24min 13s\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["%%time\n","\n","#2e map + reduce\n","training_subpatterns = combined_training_patterns.flatMap(map_to_subpatterns)\n","\n","combined_training_subpatterns = training_subpatterns.map(format_value_to_list)\n","combined_training_subpatterns = combined_training_subpatterns.reduceByKey(reduce_subpatterns)\n","\n","combined_training_subpatterns.map(format_tuples).toDF(['subpatterns', 'combined_rules']).show(50, truncate=150)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhrfpERBnilW","outputId":"cdf3107a-ced6-48fd-c0f2-134f1b407979"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|                                                                                                          patterns|                                                                                                                                                                                                                                                                                           association_rules|\n","+------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|                                                                 (\"Organic D'Anjou Pears\", 'Organic Mango Chunks')|[('Organic Extra Firm Tofu', 0.08333333333333333), ('Vanilla Pure Almond Milk', 0.08333333333333333), ('Organic Peanut Butter Granola', 0.08333333333333333), ('Synergy Organic & Raw Cosmic Cranberry', 0.08333333333333333), ('Organic Blueberries', 0.08333333333333333), ('Total 2% All Natural Low F...|\n","|                                                                       ('Limes', 'Organic Red On the Vine Tomato')|[('Organic Red Onion', 0.17857142857142858), ('Honeycrisp Apple', 0.03571428571428571), ('Chocolate Mint Miracle Tart', 0.03571428571428571), ('Coconut Milk Vanilla Frozen Dessert', 0.03571428571428571), ('Lasagna, Vegetable, with Daiya Cheeze', 0.03571428571428571), ('Guacamole', 0.0714285714285...|\n","|                                                       ('French Style Fresh Cut Green Beans', 'Honeycrisp Apples')|[('Cheese Finely Shredded Mexican Four Cheese Blend', 1.0), ('Whole Kernel Corn No Salt Added', 1.0), ('Caramel Macchiato Gourmet Coffee Creamer', 1.0), ('Presliced Everything Bagels', 1.0), ('Yellow Onions', 1.0), ('Daybreak Morning Blend Light Roast Ground Coffee', 1.0), ('Organic Fat Free Milk...|\n","|('Cheese Stuffed Crust Four Cheese (Mozzarella, Parmesan, Asiago and Romano) Pizza', 'Vanilla Almond Fit Granola')|[('White Cauliflower', 1.0), ('Alpine Spring Water', 1.0), ('Unsalted Butter', 1.0), ('Pure Vanilla Extract', 1.0), ('Vegetable Oil & Nonfat Yogurt Spread', 1.0), ('Versatile Stain Remover 65 Loads', 1.0), ('Taco Seasoned Ground Turkey', 1.0), ('Lemongrass Long Lasting Deodorant Stick', 1.0), ('O...|\n","|                                            ('Ground Smoked Paprika', 'Organic Quinoa Cacao Sprouted Oat Granola')|[('Organic Ketchup Gluten Free', 1.0), ('Chickpeas! Pasta Shells', 1.0), ('Sushi Nori, Pacific, Organic', 1.0), ('Organic Veganaise', 1.0), ('Organic White Onions', 1.0), ('Original Vegan Alfredo Sauce', 1.0), ('Organic Mung-Bean Sprouts', 1.0), (\"Dairy Free Gluten Free Cheeze Lover's Pizza\", 1.0...|\n","+------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","only showing top 5 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 117:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 45.8 ms, sys: 17.8 ms, total: 63.5 ms\n","Wall time: 4min 8s\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["%%time\n","\n","#Map final\n","assoc_rules_training_set = combined_training_subpatterns.flatMap(map_to_assoc_rules)\n","assoc_rules_training_set.map(format_tuples).toDF(['patterns', 'association_rules']).show(5, truncate=300)\n","assoc_rules_training_set.map(format_tuples).toDF(['patterns', 'association_rules']).createOrReplaceTempView(\"assoc_rules_training_set\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yGsKSxPO5rQ","outputId":"baf74219-87f2-43c8-b8fa-76a08899da96"},"outputs":[{"name":"stdout","output_type":"stream","text":["Produit sélectionné au hasard : ID = 47839, Clé du produit = ('100% Pure Grapefruit Joyful Essential Oil',)\n"]}],"source":["#Test sélection d'un produit au hasard et affichage des informations des produits associés\n","\n","#Sélection d'un produit au hasard\n","random_product = spark.sql('SELECT p.product_id, product_name FROM order_prod op JOIN products p ON op.product_id = p.product_id ORDER BY RAND() LIMIT 1').rdd.collect()\n","\n","#Formattage en tuple\n","random_product_key = \"(\\'\" + random_product[0].product_name + \"\\',)\"\n","random_product_id = str(random_product[0].product_id)\n","print(\"Produit sélectionné au hasard : ID = \" + random_product_id +\", Clé du produit = \" + random_product_key)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BR_JgWh4nilW","outputId":"0f138c11-6bfc-47bf-fd28-e81a8adcdca2"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["('100% Pure Grapefruit Joyful Essential Oil',)\n"]},{"name":"stderr","output_type":"stream","text":["Exception in thread \"serve RDD 325 with partitions 0\" java.net.SocketTimeoutException: Accept timed out\n","\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n","\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n","\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n","\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n","[Stage 148:=============================================>         (10 + 2) / 12]\r"]},{"name":"stdout","output_type":"stream","text":["+----------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|                                      patterns|                                                                                                                                     association_rules|\n","+----------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|('100% Pure Grapefruit Joyful Essential Oil',)|[('Authentic French Brioche', 0.25), ('Sea Salt Flakes Original', 0.25), ('Organic Whole Milk with DHA Omega-3', 0.25), ('Coconut  Chocolate Bar', ...|\n","+----------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["print(random_product_key)\n","#On crée une requête paramétrique qui prend en argument le produit aléatoire\n","output_table = spark.sql('SELECT *'\n","          ' FROM assoc_rules_training_set a'\n","          ' WHERE patterns LIKE \"' + random_product_key + '\\\"')\n","\n","output_table.show(5,truncate=150)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"me3AnorhnilW","outputId":"5e34e4b8-4e7f-40a4-e7a6-9f4ab2be949e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Produit sélectionné au hasard : ('100% Pure Grapefruit Joyful Essential Oil',)\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 172:==================================================>    (11 + 1) / 12]\r"]},{"name":"stdout","output_type":"stream","text":["+---------------------------------------------+----------+\n","|                                 product_name|confidence|\n","+---------------------------------------------+----------+\n","|                     Authentic French Brioche|      25.0|\n","|                     Sea Salt Flakes Original|      25.0|\n","|          Organic Whole Milk with DHA Omega-3|      25.0|\n","|                       Coconut  Chocolate Bar|      25.0|\n","|            Organic Garnet Sweet Potato (Yam)|      25.0|\n","|                       Tea Tree Essential Oil|      25.0|\n","|                        Organic D'Anjou Pears|      25.0|\n","|         Grade A Large Eggs Cage Free Omega 3|      25.0|\n","|                Natural Premium Coconut Water|      25.0|\n","|                                   Facial Oil|      25.0|\n","|                           Premium Epsom Salt|      25.0|\n","|                         Organic Hass Avocado|      25.0|\n","|  100% Pure Tangerine Cheering Essential Oil,|      25.0|\n","|                       Witch Hazel Astringent|      25.0|\n","|                         Organic Baby Arugula|      25.0|\n","|                               Organic Garlic|      25.0|\n","|                     Organic Dandelion Greens|      25.0|\n","|                      Organic Extra Firm Tofu|      25.0|\n","|                             100% Mango Juice|      25.0|\n","|                           Organic Pear Juice|      25.0|\n","|                    Organic Sunflower Kernels|      25.0|\n","|                   Frozen Organic Blueberries|      25.0|\n","|                     Organic Broccoli Florets|      25.0|\n","|                    Organic Coconut Snack Bar|      25.0|\n","|                                Organic Lemon|      25.0|\n","|                                Blood Oranges|      25.0|\n","|                             Large Grapefruit|      25.0|\n","|                                   Jojoba Oil|      25.0|\n","|                   Organic Small Bunch Celery|      25.0|\n","|                           Organic Egg Whites|      25.0|\n","|                         Organic Blackberries|      25.0|\n","|   Natural Vitamin E, Skin Beauty Oil 9000 IU|      25.0|\n","|                                  Large Lemon|      50.0|\n","|                             Lacinato Kale Og|      25.0|\n","|                         Organic Strawberries|      25.0|\n","|                                Aluminum Foil|      25.0|\n","|                                        Limes|      25.0|\n","|             Fair Trade Unscented Shea Butter|      25.0|\n","|                          Large Alfresco Eggs|      25.0|\n","|Body Deodorant Stick Fragrance & Paraben Free|      25.0|\n","|                              Basil Dish Soap|      25.0|\n","|                                Bing Cherries|      25.0|\n","|              Water Stop Premium Gloves Small|      25.0|\n","|                        Blueberry Protein Bar|      25.0|\n","|           Natural Cedar Scent Toilet Cleaner|      25.0|\n","|                       Organic Sliced Peaches|      25.0|\n","|                         100% Pure Jojoba Oil|      25.0|\n","|             Organic Brown Rice Pasta Spirals|      25.0|\n","|                             100% Guava Juice|      25.0|\n","|                   Sparkling Water Grapefruit|      25.0|\n","+---------------------------------------------+----------+\n","only showing top 50 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["output_table_rdd = output_table.rdd\n","print(\"Produit sélectionné au hasard : \" + random_product_key)\n","\n","def map_to_table(row):\n","    #On transforme la string en liste automatiquement\n","    product_list = eval(row.association_rules)\n","    for product_tuple in product_list:\n","        product_name, order_probability = product_tuple\n","        yield(product_name, float(order_probability)*100)\n","\n","output_table_rdd = output_table_rdd.flatMap(map_to_table)\n","\n","output_table_rdd.map(format_tuples).toDF(['product_name','confidence']).show(50,truncate=150)"]},{"cell_type":"markdown","metadata":{"id":"5DkNPEtGMa0l"},"source":["## 4. MBA pour le dataset complet / MBA for the full dataset (20 points)\n","\n","Comme vous l'avez probablement remarqué, même pour un ensemble de données moins volumineux (le training dataset ne contient que 131 000 commandes), l'algorithme MBA est coûteux en calcul. Pour cette raison, cette fois, nous allons répéter le processus, mais en utilisant maintenant Google Cloud Platform (GCP) pour créer un grand cluster. Toutes les instructions pour créer un cluster avec spark et comment soumettre un travail seront expliquées dans le laboratoire. Dans tous les cas, vous devez lire les instructions données dans le ``Instruction_GCP.pdf``.\n","\n","Cette fois, nous travaillerons avec le fichier ``order_products__prior.csv``, qui contient plus de 3M commandes.\n","\n","**PRODUCTION ATTENDUE**\n","\n","Après avoir exécuté le MBA pour la plus grande collection de commandes, sélectionnez au hasard UN produit acheté dans ``order_products__prior`` et affichez les règles d'association (nom du produit et valeur d'association) de ce produit, c'est-à-dire lorsque le produit est seul dans le panier. La sortie doit être formatée dans un tableau, où chaque ligne contenant les informations d'un produit associé. \n","\n","- Affichez l'ID et le nom du produit sélectionné au hasard.\n","- Signaler le temps d'exécution.\n","\n","**Remarque importante : joignez des captures d'écran de votre sortie et de votre configuration de cluster.** \n","___\n","As you probably noticed, even for a not so large data set (the training file has only 131K orders), the MBA algorithm is computationally expensive. For that reason, this time, we will repeat the process, but now using the Google Cloud Platform (GCP) to create a large computer cluster. All the instructions for creating a computing cluster with spark and how to submit a job will be explained in both sessions of the laboratory. In any case, you should read the instructions given in the ``Instruction_GCP.pdf``.\n","\n","This time, we will work with the ``order_products__prior.csv`` file, which contains more than 3M orders.\n","\n","**EXPECTED OUTPUT**\n","\n","After you ran the MBA for the larger collection of orders, randomly select ONE product purchased in ``order_products__prior`` and print the association rules (product name and association value) of this product, i.e., when the product is alone in the basket. The output should be formatted in a table, where each row containing the information of one associated product.\n","\n","- Print both ID and Name of the random selected product.\n","- Report the execution time.\n","\n","**Important note: Attach screenshots from your output and configuration of your cluster.** "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KI_-6rTOO5rR","outputId":"3c9fcb9b-00d5-47e6-9b6c-01f707957d57"},"outputs":[{"name":"stdout","output_type":"stream","text":["Produit sélectionné au hasard : ID = 3583, Clé du produit = ('Unsweetened Coconut Milk Beverage',)\n","CPU times: user 4.55 ms, sys: 666 µs, total: 5.22 ms\n","Wall time: 300 ms\n"]}],"source":["%%time\n","\n","#Sélection d'un produit au hasard\n","random_product = spark.sql('SELECT p.product_id, product_name FROM order_prod op JOIN products p ON op.product_id = p.product_id ORDER BY RAND() LIMIT 1').rdd.collect()\n","\n","#Formattage en tuple\n","random_product_key = \"(\\'\" + random_product[0].product_name + \"\\',)\"\n","random_product_id = str(random_product[0].product_id)\n","print(\"Produit sélectionné au hasard : ID = \" + random_product_id +\", Clé du produit = \" + random_product_key)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1cVWxraMa0l","outputId":"569419f4-3666-4b22-985b-8d140420fcbc"},"outputs":[{"name":"stderr","output_type":"stream","text":["Exception in thread \"serve RDD 418 with partitions 0\" java.net.SocketTimeoutException: Accept timed out\n","\tat java.base/java.net.PlainSocketImpl.socketAccept(Native Method)\n","\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\n","\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\n","\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\n","\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:64)\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Produit sélectionné au hasard : ('Unsweetened Coconut Milk Beverage',)\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 192:==================================================>    (11 + 1) / 12]\r"]},{"name":"stdout","output_type":"stream","text":["+---------------------------------------------------------+--------------------+\n","|                                             product_name|          confidence|\n","+---------------------------------------------------------+--------------------+\n","|                                          Garlic Couscous|  0.1707455890722823|\n","|                   Chamomile & Calendula Calming Baby Oil|0.056915196357427436|\n","|                                  Hearty Nine Grain Bread| 0.11383039271485487|\n","|                                 Hot Oatmeal Variety Pack| 0.11383039271485487|\n","|                                           Chik'n Patties|  0.1707455890722823|\n","|                                              Hard Salami|0.056915196357427436|\n","|                        Two Organic Cornmeal Pizza Crusts|0.056915196357427436|\n","|                                    Cheddar Cheese Medium| 0.11383039271485487|\n","|                               Organic Roast Turkey Gravy|0.056915196357427436|\n","|           Ranch Style Beef & Vegetable Recipe Dog Treats|0.056915196357427436|\n","|                   Soy Free Buttery Spread with Olive Oil| 0.28457598178713717|\n","|                                  Fish Sticks Family Pack|0.056915196357427436|\n","|                        Sparkling Probiotic Drink Coconut| 0.11383039271485487|\n","|                                      Hawaiian Body Scrub|0.056915196357427436|\n","|                                         Red Leaf Lettuce|  0.4553215708594195|\n","|                  All-In-One Nutritional Shake, Chocolate| 0.11383039271485487|\n","|                                     Mild Tomatillo Salsa| 0.11383039271485487|\n","|                               Organic Multigrain Waffles| 0.28457598178713717|\n","|                                            Spinach Pizza| 0.22766078542970974|\n","|                                    Organic String Cheese| 0.11383039271485487|\n","|                      Organic Promise Autumn Wheat Cereal| 0.28457598178713717|\n","|                                  Currant + Mint Lamb Bar| 0.11383039271485487|\n","|                                   Cheese Square Crackers| 0.11383039271485487|\n","|                      Organic Earl Grey Black Tea Sachets|0.056915196357427436|\n","|                                   100% Lactose Free Milk|0.056915196357427436|\n","|                                     100% Cranberry Juice|0.056915196357427436|\n","|                                   Diced Butternut Squash|0.056915196357427436|\n","|                        Boneless Skinless Chicken Breasts|  2.7319294251565167|\n","|                          Gluten Free Classic Hotdog Buns| 0.11383039271485487|\n","|                     Hint of Lime Flavored Tortilla Chips|0.056915196357427436|\n","|                                 Plain Almond Milk Yogurt|  0.4553215708594195|\n","|      Almond Breeze Unsweetened Almond Coconut Milk Blend|  0.1707455890722823|\n","|                          Organic Spicy Ginger Herbal Tea|0.056915196357427436|\n","|                                             Kelp Noodles|0.056915196357427436|\n","|                       Cheese Natural Extra Sharp Cheddar| 0.11383039271485487|\n","|                                                 All-In-1|0.056915196357427436|\n","|                                  Multigrain Penne Rigate|0.056915196357427436|\n","|           Choose-A-Size White Paper Towels 8 Giant Rolls|  0.1707455890722823|\n","|Fat Free Smart Ground Mexican Style Taco Protein Crumbles|0.056915196357427436|\n","|                            Oragnic Ginger Lemon Kombucha|0.056915196357427436|\n","|  Honey Bunches of Oats Honey Roasted with Almonds Cereal| 0.22766078542970974|\n","|                   Hearty Minestrone with Vegetables Soup|0.056915196357427436|\n","|               Blueberry Pecan Plus Fiber Fruit & Nut Bar|0.056915196357427436|\n","|                               Frozen Goo-Berry Pie Kefir|0.056915196357427436|\n","|                   Double Smoked Uncured Center Cut Bacon|0.056915196357427436|\n","|                 XX Espresso Iced Coffee With Almond Milk| 0.11383039271485487|\n","|                        Bite Size Cheddar Cheese Crackers|0.056915196357427436|\n","|     Baguette, Pretzel, European Style, Butter & Sea Salt|0.056915196357427436|\n","|                                                   Matcha|0.056915196357427436|\n","|                                    Sea Salt Potato Chips|   0.512236767216847|\n","+---------------------------------------------------------+--------------------+\n","only showing top 50 rows\n","\n","CPU times: user 989 ms, sys: 260 ms, total: 1.25 s\n","Wall time: 1h 58min 59s\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["%%time\n","\"\"\"\n","TODO: create a query to create and sctruct the transactions from the order_products__prior.csv file\n","\"\"\"\n","def map_to_table(row):\n","    #On transforme la string en liste automatiquement\n","    product_list = eval(row.association_rules)\n","    for product_tuple in product_list:\n","        product_name, order_probability = product_tuple\n","        yield(product_name, float(order_probability)*100)\n","\n","#Chargement du fichier prior\n","df_order_prod_prior = spark.read.csv('gs://fd_tp2/instacart/order_products__prior.csv', header=True, sep=',', inferSchema=True)\n","#Création de la table order_prod_prior\n","df_order_prod_prior.createOrReplaceTempView(\"order_prod_prior\")\n","\n","\n","#Chargement des transactions\n","prior_set_df = spark.sql('SELECT order_id, CONCAT_WS(\";\",COLLECT_LIST(p.product_name)) AS transaction'\n","          ' FROM order_prod_prior opp'\n","          ' JOIN products p ON p.product_id = opp.product_id'\n","          ' GROUP BY order_id'\n",")\n","\n","\"\"\"\n","TODO: run the MBA algorithm and print the requested output\n","\"\"\"\n","#1er map + reduce\n","prior_set_rdd = prior_set_df.rdd\n","prior_patterns = prior_set_rdd.flatMap(map_to_patterns)\n","combined_prior_patterns = prior_patterns.reduceByKey(reduce_patterns)\n","\n","#2e map + reduce\n","prior_subpatterns = combined_prior_patterns.flatMap(map_to_subpatterns)\n","combined_prior_subpatterns = prior_subpatterns.map(format_value_to_list)\n","combined_prior_subpatterns = combined_prior_subpatterns.reduceByKey(reduce_subpatterns)\n","\n","#combined_prior_subpatterns.map(format_tuples).toDF(['subpatterns', 'combined_rules']).show(10, truncate=150)\n","\n","#Map final\n","assoc_rules_prior_set = combined_prior_subpatterns.flatMap(map_to_assoc_rules)\n","#assoc_rules_prior_set.map(format_tuples).toDF(['patterns', 'association_rules']).show(5, truncate=300)\n","\n","#Formattage en table\n","assoc_rules_prior_set.map(format_tuples).toDF(['patterns', 'association_rules']).createOrReplaceTempView(\"assoc_rules_prior_set\")\n","\n","print(\"Produit sélectionné au hasard : \" + random_product_key)\n","\n","#Sélection de la ligne correspondante\n","output = spark.sql('SELECT *'\n","          ' FROM assoc_rules_prior_set ap'\n","          ' WHERE patterns LIKE \"' + random_product_key + '\\\"')\n","\n","#Formattage de la sortie en table\n","output_rdd = output.rdd\n","output_table_rdd = output_rdd.flatMap(map_to_table)\n","output_table_rdd.map(format_tuples).toDF(['product_name','confidence']).show(50,truncate=200)\n","\n","\n"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"a3e1557498e5254914fa9d3d4c811555ae0601fcd06273328e2e97921c0b8766"}}},"nbformat":4,"nbformat_minor":0}